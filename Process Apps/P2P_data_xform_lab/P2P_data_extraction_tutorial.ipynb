{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Coding a Process App that uses several data sets from a ZIP file\n",
    "This tutorial is using the simple Procure to Pay data set that could be extracted from systems like SAP.\n",
    "\n",
    "Instead of connecting the process app to an external system, we leverage data tables that have been extracted previously and that are going to be transformed to create an event log for IBM Process Mining.\n",
    "\n",
    "The full source code of the python code used to create this app is here: [P2P_data_xform_lab.py](./P2P_data_xform_lab.py).\n",
    "\n",
    "## Data Sources\n",
    "When launching the process app, the user is asked to upload a ZIP file that contains the following files:\n",
    "- requisition.csv\n",
    "- procurement.csv\n",
    "- invoice.csv\n",
    "\n",
    "requisition.csv contains a row for each requisition uniquely identified with a Req_ID. Along with many attributes, two series of columns are interesting as they store important milestones of the requisition: the creation date and the release date. We will create a process mining event (activity) for the creation and for the release of each requisition.\n",
    "\n",
    "procurement.csv is a ready-to-use event log for process mining that includes all the P2P events except the requisitions. Each procurement case includes several events such as 'Order Item Created', 'Invoice Registered', and many more.  \n",
    "\n",
    "invoice.csv contains information about each invoice that we would like to add into each procurement process that matches the Invoice_ID.\n",
    "\n",
    "## Transformation Process\n",
    "In this process app python code, we are going to:\n",
    "- Extract the 3 csv files from the ZIP file uploaded by the user\n",
    "- Transform requisition.csv into a process mining event log\n",
    "- Keep procurement.csv that is a ready-to-use event log for process mining\n",
    "- Merge (left join) invoice.csv with procurement.csv to add invoice information that is missing from procurement.csv\n",
    "- Concatenate requisition and procurement events to form a multilevel process (2 columns used as case IDs: Req_ID and Proc_ID)\n",
    "\n",
    "## Python Code Template\n",
    "IBM Process Mining will call the ```execute(context)``` function when starting the data transformation of the process app. This is where we are doing most of the data transformation work.\n",
    "\n",
    "The context typically contains the user input parameters, and the name of the ZIP file that the user uploaded.\n",
    "\n",
    "To run this program on you local machine with this Jupyter Notebook, you need to install pandas and zipfile:\n",
    "```shell\n",
    "pip install pandas\n",
    "``````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "import time\n",
    "\n",
    "def execute(context):\n",
    "    # Unzip the file that includes the 3 files that we need to transform and merge\n",
    "    myFileUploadName = context[\"fileUploadName\"]\n",
    "    with ZipFile(myFileUploadName, 'r') as f:\n",
    "        f.extractall()\n",
    "\n",
    "\n",
    "context = {'fileUploadName':'P2P.zip'}\n",
    "execute(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you execute the code above, you should see that you now have the 3 CSV files extracted in your local directory. \n",
    "IBM Process Mining executes this code from a directory created specifically for the process app, that contains the python file and the uploaded ZIP file. This is where the 3 files are extracted.\n",
    "\n",
    "## Using Pandas to create and manipulate data tables\n",
    "Pandas is a powerful table data manipulation library used by thousands of data engineers: https://pandas.pydata.org/docs/.\n",
    "We are going to use Pandas dataframes to store and manipulate the data loaded from the CSV files.\n",
    "\n",
    "At the end of the ```execute()``` function, we need to return a Pandas dataframe that contains the event logs.\n",
    "\n",
    "## Start Simple: Load the procurement file\n",
    "Note that we wait a bit that the files are extracted before loading them as Pandas dataframes.\n",
    "Then we just need to call the Pandas ```read_csv()``` method that loads the procurement event log into a dataframe.\n",
    "At that stage, that's the dataframe that the ```execute()``` function returns for the process app, and that could be good enough if we wanted to just mine the procurement process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "import time\n",
    "\n",
    "def execute(context):\n",
    "    # Unzip the file that includes the 3 files that we need to transform and merge\n",
    "    myFileUploadName = context[\"fileUploadName\"]\n",
    "    with ZipFile(myFileUploadName, 'r') as f:\n",
    "        f.extractall()\n",
    "\n",
    "    time.sleep(5)\n",
    "    procurements = pd.read_csv('./procurement.csv', low_memory=False)\n",
    "    return procurements\n",
    "\n",
    "\n",
    "context = {'fileUploadName':'P2P.zip'}\n",
    "procurements = execute(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment a few useful Pandas methods.\n",
    "Experiment a few common pandas methods on dataframes. The code cell below displays the result of the last code line. You can comment or move the lines.\n",
    "This is just to show a abstract of how we can handle tables with Pandas, with methods that are often used in our data transformation process.\n",
    "You are stronghly encouraged to study https://pandas.pydata.org/docs/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Vendor known from IT10', 'Vendor known from IT11',\n",
       "       'Vendor unknown from IT10', 'Vendor unknown from IT11', nan],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "procurements = pd.read_csv('./procurement.csv', low_memory=False)\n",
    "procurements.columns # list of columns\n",
    "procurements.describe() # statistics of the dataframe\n",
    "procurements['Order_Vendor'] # Column of Order_Vendor\n",
    "procurements['Order_Vendor'].unique() # List of Order_Vendor \n",
    "len(procurements['Order_Vendor'].unique()) # Number of distinct Order_Vendor\n",
    "procurements['Order_Vendor'].isna() # test if 'Order_Vendor' is NaN\n",
    "procurements[procurements['Order_Vendor'].isna()] # dataframe keeps procurements where 'Order_Vendor' is NaN\n",
    "procurements[~procurements['Order_Vendor'].isna()] # dataframe keeps procurements where 'Order_Vendor' is not NaN\n",
    "procurements['NEW_DIM_1'] = 'A VALUE' # adds a column with a value\n",
    "procurements['NEW_DIM_2'] = procurements['Order_Vendor'] # set a column with the value of another column\n",
    "procurements.rename(columns={'NEW_DIM_1':'DIM_3', 'NEW_DIM_2':'DIM_4'}) # rename columns in a copy of the df\n",
    "# The next 3 commands are bound\n",
    "procurements.rename(columns={'NEW_DIM_1':'DIM_3', 'NEW_DIM_2':'DIM_4'}, inplace=True) # rename columns in the original df\n",
    "procurements['DIM_4'] # list column DIM_4\n",
    "procurements.drop(columns=['DIM_3', 'DIM_4'], inplace=True) # Delete columns DIM_3 and DIM_4\n",
    "\n",
    "procurements.iloc[1:4] # Select rows 1 to 4\n",
    "\n",
    "# The next 3 commands are bound\n",
    "# Creating a new column 'DIM_5' which value depends on a test in one column, and the value built from another\n",
    "procurements.loc[procurements['Order_Vendor'].isna(),'DIM_5'] = 'Vendor unknown from ' + procurements['Purchase_Organization']\n",
    "procurements.loc[~procurements['Order_Vendor'].isna(),'DIM_5'] = 'Vendor known from ' + procurements['Purchase_Organization']\n",
    "procurements['DIM_5'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformation of the requisition.csv table\n",
    "The requisition table stores each requisition in a single row that includes both the creation information and the release information. We create an event log that contains two activities: creation and release.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Req_Header', 'Req_Line', 'Req_ID', 'Create_Date', 'Create_User',\n",
       "       'Create_Role', 'Create_Type', 'Create_Source', 'Release_DateTime',\n",
       "       'Release_User', 'Release_Role', 'Release_Type', 'Release_Source'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requisitions = pd.read_csv('./requisition.csv', low_memory=False)\n",
    "requisitions.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requisition Created Activity\n",
    "The create_requisitions event log is a copy of the requisitions dataframe. We rename some columns, and we drop the columns related to the release activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Req_Header</th>\n",
       "      <th>Req_Line</th>\n",
       "      <th>Req_ID</th>\n",
       "      <th>datetime</th>\n",
       "      <th>user</th>\n",
       "      <th>role</th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>02a9abba-ca02-48c6-a77d-448d9a22bdd5</td>\n",
       "      <td>2019-01-09</td>\n",
       "      <td>USR01957</td>\n",
       "      <td>Job System</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Requisition Created</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>057f6784-3b2d-4e4a-b5a5-5d27e12fbbbc</td>\n",
       "      <td>2019-01-09</td>\n",
       "      <td>USR01851</td>\n",
       "      <td>Job System</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Requisition Created</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0609f016-58e9-4d5e-8298-3d2c33fe6769</td>\n",
       "      <td>2019-01-16</td>\n",
       "      <td>ACQ04</td>\n",
       "      <td>Job System</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Requisition Created</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>07a5950c-fdf4-4cb0-a54c-4ade6d10414b</td>\n",
       "      <td>2019-01-07</td>\n",
       "      <td>USR00245</td>\n",
       "      <td>Job System</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Requisition Created</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>090ee711-f0ec-4a69-b202-e62e8a441812</td>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>USR00193</td>\n",
       "      <td>Job System</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Requisition Created</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24128</th>\n",
       "      <td>90000137.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0090000137_30</td>\n",
       "      <td>2018-11-28</td>\n",
       "      <td>MKT103</td>\n",
       "      <td>Secretary</td>\n",
       "      <td>Indirect</td>\n",
       "      <td>N.A.</td>\n",
       "      <td>Requisition Created</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24129</th>\n",
       "      <td>90000137.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0090000137_40</td>\n",
       "      <td>2018-11-28</td>\n",
       "      <td>MKT103</td>\n",
       "      <td>Secretary</td>\n",
       "      <td>Indirect</td>\n",
       "      <td>N.A.</td>\n",
       "      <td>Requisition Created</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24130</th>\n",
       "      <td>90000137.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0090000137_50</td>\n",
       "      <td>2018-11-28</td>\n",
       "      <td>MKT103</td>\n",
       "      <td>Secretary</td>\n",
       "      <td>Indirect</td>\n",
       "      <td>N.A.</td>\n",
       "      <td>Requisition Created</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24131</th>\n",
       "      <td>90000137.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0090000137_60</td>\n",
       "      <td>2018-11-28</td>\n",
       "      <td>MKT103</td>\n",
       "      <td>Secretary</td>\n",
       "      <td>Indirect</td>\n",
       "      <td>N.A.</td>\n",
       "      <td>Requisition Created</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24132</th>\n",
       "      <td>90000137.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0090000137_90</td>\n",
       "      <td>2018-11-28</td>\n",
       "      <td>MKT103</td>\n",
       "      <td>Secretary</td>\n",
       "      <td>Indirect</td>\n",
       "      <td>N.A.</td>\n",
       "      <td>Requisition Created</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24133 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Req_Header  Req_Line                                Req_ID    datetime  \\\n",
       "0             NaN       NaN  02a9abba-ca02-48c6-a77d-448d9a22bdd5  2019-01-09   \n",
       "1             NaN       NaN  057f6784-3b2d-4e4a-b5a5-5d27e12fbbbc  2019-01-09   \n",
       "2             NaN       NaN  0609f016-58e9-4d5e-8298-3d2c33fe6769  2019-01-16   \n",
       "3             NaN       NaN  07a5950c-fdf4-4cb0-a54c-4ade6d10414b  2019-01-07   \n",
       "4             NaN       NaN  090ee711-f0ec-4a69-b202-e62e8a441812  2019-01-10   \n",
       "...           ...       ...                                   ...         ...   \n",
       "24128  90000137.0      30.0                         0090000137_30  2018-11-28   \n",
       "24129  90000137.0      40.0                         0090000137_40  2018-11-28   \n",
       "24130  90000137.0      50.0                         0090000137_50  2018-11-28   \n",
       "24131  90000137.0      60.0                         0090000137_60  2018-11-28   \n",
       "24132  90000137.0      90.0                         0090000137_90  2018-11-28   \n",
       "\n",
       "           user        role      type source             activity  \n",
       "0      USR01957  Job System       NaN    NaN  Requisition Created  \n",
       "1      USR01851  Job System       NaN    NaN  Requisition Created  \n",
       "2         ACQ04  Job System       NaN    NaN  Requisition Created  \n",
       "3      USR00245  Job System       NaN    NaN  Requisition Created  \n",
       "4      USR00193  Job System       NaN    NaN  Requisition Created  \n",
       "...         ...         ...       ...    ...                  ...  \n",
       "24128    MKT103   Secretary  Indirect   N.A.  Requisition Created  \n",
       "24129    MKT103   Secretary  Indirect   N.A.  Requisition Created  \n",
       "24130    MKT103   Secretary  Indirect   N.A.  Requisition Created  \n",
       "24131    MKT103   Secretary  Indirect   N.A.  Requisition Created  \n",
       "24132    MKT103   Secretary  Indirect   N.A.  Requisition Created  \n",
       "\n",
       "[24133 rows x 9 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_requisitions = requisitions.copy()\n",
    "create_requisitions['activity'] = 'Requisition Created'\n",
    "create_requisitions.rename(columns={'Create_Date': 'datetime', 'Create_User': 'user',\n",
    "                        'Create_Role': 'role', 'Create_Type': 'type', 'Create_Source': 'source'}, inplace=True)\n",
    "create_requisitions.drop(['Release_DateTime', 'Release_User', 'Release_Role',\n",
    "                        'Release_Type', 'Release_Source'], axis=1, inplace=True)\n",
    "create_requisitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requisition Released Activity\n",
    "The release_requisitions event log is also  a copy of the requisitions dataframe. We rename some columns, and we drop the columns related to the creation activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Req_Header', 'Req_Line', 'Req_ID', 'datetime', 'user', 'role', 'type',\n",
       "       'source', 'activity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "release_requisitions = requisitions.copy()\n",
    "release_requisitions['activity'] = 'Requisition Released'\n",
    "release_requisitions.rename(columns={'Release_DateTime': 'datetime', 'Release_User': 'user',\n",
    "                        'Release_Role': 'role', 'Release_Type': 'type', 'Release_Source': 'source'}, inplace=True)\n",
    "release_requisitions.drop(['Create_Date', 'Create_User', 'Create_Role',\n",
    "                            'Create_Type', 'Create_Source'], axis=1, inplace=True)\n",
    "# If some requisition cases are not complete, we remove rows where the requisition release date is NaN, as in\n",
    "# this case, the activity did not yet occur. This is actually never the case.\n",
    "release_requisitions = release_requisitions[release_requisitions['datetime'].notna()]\n",
    "release_requisitions.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Invoice information to the procurement event log\n",
    "The last transformation consists in appending the invoice details that are stored in the invoice.csv file, to the procurement event log.\n",
    "In the procurement event log, we only have the Invoice_ID, and we have no information about the amount or the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Req_ID', 'PO_Header', 'PO_Line', 'PO_ID', 'MatDoc_Header',\n",
       "       'MatDoc_Line', 'MatDoc_Year', 'MatDoc_ID', 'gr_h_y', 'Invoice_ID',\n",
       "       'activity', 'datetime', 'user', 'role', 'rses_h', 'rses_l', 'rses_y',\n",
       "       'mandt', 'bukrs', 'xblnr', 'fl_h', 'fl_y', 'value_old', 'value_new',\n",
       "       'clear_doc', 'qmnum', 'data_gr_effettiva', 'usertype', 'Order_Type',\n",
       "       'Purchasing_Group', 'Purch_Group_Type', 'Material_Group_Area',\n",
       "       'Accounting_Type', 'Order_Vendor', 'Order_Source', 'Department',\n",
       "       'Order_Amount', 'Material', 'lead_time_material', 'Material_Type',\n",
       "       'Purch_Group_Area', 'Requisition_Plant', 'Order_Plant',\n",
       "       'Material_Plant', 'data_gr_ordine', 'data_gr_stat', 'data_gr_ipo',\n",
       "       'Paid_Amount', 'Paid_Vendor', 'split_ordine', 'split_riga_ordine',\n",
       "       'missmatch_riga_oda', 'check_riga_gagm', 'consegna_ipotetica',\n",
       "       'consegna_oda_ipotetica', '_consegna_statistica_ipotetica_',\n",
       "       'pay_delay', 'pay_type', 'Req_Required_Vendor', 'Material_Group',\n",
       "       'Invoice_Date', 'Requisition_Vendor', 'Purchase_Organization',\n",
       "       'insert_date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "procurements = pd.read_csv('./procurement.csv', low_memory=False)\n",
    "procurements.columns # list of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The invoice table contains these invoice information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Invoice_ID', 'Invoice_Header', 'Invoice_Year', 'Invoice_Amount',\n",
       "       'Invoice_Vendor', 'Invoice_Due_Date', 'Invoice_Vendor_City'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoices = pd.read_csv('./invoice.csv')\n",
    "invoices.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were using a database, we would join-left the two table using the Invoice_ID field. Pandas provides the same feature using the merge method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Req_ID', 'PO_Header', 'PO_Line', 'PO_ID', 'MatDoc_Header',\n",
       "       'MatDoc_Line', 'MatDoc_Year', 'MatDoc_ID', 'gr_h_y', 'Invoice_ID',\n",
       "       'activity', 'datetime', 'user', 'role', 'rses_h', 'rses_l', 'rses_y',\n",
       "       'mandt', 'bukrs', 'xblnr', 'fl_h', 'fl_y', 'value_old', 'value_new',\n",
       "       'clear_doc', 'qmnum', 'data_gr_effettiva', 'usertype', 'Order_Type',\n",
       "       'Purchasing_Group', 'Purch_Group_Type', 'Material_Group_Area',\n",
       "       'Accounting_Type', 'Order_Vendor', 'Order_Source', 'Department',\n",
       "       'Order_Amount', 'Material', 'lead_time_material', 'Material_Type',\n",
       "       'Purch_Group_Area', 'Requisition_Plant', 'Order_Plant',\n",
       "       'Material_Plant', 'data_gr_ordine', 'data_gr_stat', 'data_gr_ipo',\n",
       "       'Paid_Amount', 'Paid_Vendor', 'split_ordine', 'split_riga_ordine',\n",
       "       'missmatch_riga_oda', 'check_riga_gagm', 'consegna_ipotetica',\n",
       "       'consegna_oda_ipotetica', '_consegna_statistica_ipotetica_',\n",
       "       'pay_delay', 'pay_type', 'Req_Required_Vendor', 'Material_Group',\n",
       "       'Invoice_Date', 'Requisition_Vendor', 'Purchase_Organization',\n",
       "       'insert_date', 'Invoice_Header', 'Invoice_Year', 'Invoice_Amount',\n",
       "       'Invoice_Vendor', 'Invoice_Due_Date', 'Invoice_Vendor_City'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "procurements = procurements.merge(invoices, on=\"Invoice_ID\", how=\"left\")\n",
    "procurements.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating the dataframes to produce the final event log\n",
    "The final step consists in concatenating the 2 requisition tables and the procurement table. \n",
    "Pandas concatenation is a union of the table columns. Therefore we end up with all the fields from each process, and we keep the common fields: 'datetime', 'user', 'role', 'type', 'source', 'activity'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Req_Header', 'Req_Line', 'Req_ID', 'datetime', 'user', 'role', 'type',\n",
       "       'source', 'activity', 'PO_Header', 'PO_Line', 'PO_ID', 'MatDoc_Header',\n",
       "       'MatDoc_Line', 'MatDoc_Year', 'MatDoc_ID', 'gr_h_y', 'Invoice_ID',\n",
       "       'rses_h', 'rses_l', 'rses_y', 'mandt', 'bukrs', 'xblnr', 'fl_h', 'fl_y',\n",
       "       'value_old', 'value_new', 'clear_doc', 'qmnum', 'data_gr_effettiva',\n",
       "       'usertype', 'Order_Type', 'Purchasing_Group', 'Purch_Group_Type',\n",
       "       'Material_Group_Area', 'Accounting_Type', 'Order_Vendor',\n",
       "       'Order_Source', 'Department', 'Order_Amount', 'Material',\n",
       "       'lead_time_material', 'Material_Type', 'Purch_Group_Area',\n",
       "       'Requisition_Plant', 'Order_Plant', 'Material_Plant', 'data_gr_ordine',\n",
       "       'data_gr_stat', 'data_gr_ipo', 'Paid_Amount', 'Paid_Vendor',\n",
       "       'split_ordine', 'split_riga_ordine', 'missmatch_riga_oda',\n",
       "       'check_riga_gagm', 'consegna_ipotetica', 'consegna_oda_ipotetica',\n",
       "       '_consegna_statistica_ipotetica_', 'pay_delay', 'pay_type',\n",
       "       'Req_Required_Vendor', 'Material_Group', 'Invoice_Date',\n",
       "       'Requisition_Vendor', 'Purchase_Organization', 'insert_date',\n",
       "       'Invoice_Header', 'Invoice_Year', 'Invoice_Amount', 'Invoice_Vendor',\n",
       "       'Invoice_Due_Date', 'Invoice_Vendor_City'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P2P_events = pd.concat([create_requisitions, release_requisitions, procurements])\n",
    "P2P_events = P2P_events.convert_dtypes()  # applying the best known types\n",
    "P2P_events.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually we reorder the columns to simplify the process mining mapping. To do so, copy the result of the columns method and reorder some."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime', 'user', 'role', 'type', 'source', 'activity', 'Req_ID',\n",
       "       'Req_Header', 'Req_Line', 'PO_Header', 'PO_Line', 'PO_ID',\n",
       "       'MatDoc_Header', 'MatDoc_Line', 'MatDoc_Year', 'MatDoc_ID', 'gr_h_y',\n",
       "       'Invoice_ID', 'rses_h', 'rses_l', 'rses_y', 'mandt', 'bukrs', 'xblnr',\n",
       "       'fl_h', 'fl_y', 'value_old', 'value_new', 'clear_doc', 'qmnum',\n",
       "       'data_gr_effettiva', 'usertype', 'Order_Type', 'Purchasing_Group',\n",
       "       'Purch_Group_Type', 'Material_Group_Area', 'Accounting_Type',\n",
       "       'Order_Vendor', 'Order_Source', 'Department', 'Order_Amount',\n",
       "       'Material', 'lead_time_material', 'Material_Type', 'Purch_Group_Area',\n",
       "       'Requisition_Plant', 'Order_Plant', 'Material_Plant', 'data_gr_ordine',\n",
       "       'data_gr_stat', 'data_gr_ipo', 'Paid_Amount', 'Paid_Vendor',\n",
       "       'split_ordine', 'split_riga_ordine', 'missmatch_riga_oda',\n",
       "       'check_riga_gagm', 'consegna_ipotetica', 'consegna_oda_ipotetica',\n",
       "       '_consegna_statistica_ipotetica_', 'pay_delay', 'pay_type',\n",
       "       'Req_Required_Vendor', 'Material_Group', 'Invoice_Date',\n",
       "       'Requisition_Vendor', 'Purchase_Organization', 'insert_date',\n",
       "       'Invoice_Header', 'Invoice_Year', 'Invoice_Amount', 'Invoice_Vendor',\n",
       "       'Invoice_Due_Date', 'Invoice_Vendor_City'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P2P_events = P2P_events[['activity', 'datetime', 'user', 'role', 'type',\n",
    "       'source', 'Req_ID','Req_Header', 'Req_Line', 'PO_Header', 'PO_Line', 'PO_ID', 'MatDoc_Header',\n",
    "       'MatDoc_Line', 'MatDoc_Year', 'MatDoc_ID', 'gr_h_y', 'Invoice_ID',\n",
    "       'rses_h', 'rses_l', 'rses_y', 'mandt', 'bukrs', 'xblnr', 'fl_h', 'fl_y',\n",
    "       'value_old', 'value_new', 'clear_doc', 'qmnum', 'data_gr_effettiva',\n",
    "       'usertype', 'Order_Type', 'Purchasing_Group', 'Purch_Group_Type',\n",
    "       'Material_Group_Area', 'Accounting_Type', 'Order_Vendor',\n",
    "       'Order_Source', 'Department', 'Order_Amount', 'Material',\n",
    "       'lead_time_material', 'Material_Type', 'Purch_Group_Area',\n",
    "       'Requisition_Plant', 'Order_Plant', 'Material_Plant', 'data_gr_ordine',\n",
    "       'data_gr_stat', 'data_gr_ipo', 'Paid_Amount', 'Paid_Vendor',\n",
    "       'split_ordine', 'split_riga_ordine', 'missmatch_riga_oda',\n",
    "       'check_riga_gagm', 'consegna_ipotetica', 'consegna_oda_ipotetica',\n",
    "       '_consegna_statistica_ipotetica_', 'pay_delay', 'pay_type',\n",
    "       'Req_Required_Vendor', 'Material_Group', 'Invoice_Date',\n",
    "       'Requisition_Vendor', 'Purchase_Organization', 'insert_date',\n",
    "       'Invoice_Header', 'Invoice_Year', 'Invoice_Amount', 'Invoice_Vendor',\n",
    "       'Invoice_Due_Date', 'Invoice_Vendor_City']]\n",
    "P2P_events.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can create a CSV file to test our event log in IBM Process Mining.\n",
    "Note that we do not save the index (row numbers).\n",
    "\n",
    "You can use this [backup file](./P2P%20LAB_2023-08-03_015726-0700.idp) to map some columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P2P_events.to_csv('P2Peventlog.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The complete source code that we upload in the process app is shown in the next cell. You will need to upload an idp file that contains at least the column mapping. This [backup file](./P2P%20LAB_2023-08-03_015726-0700.idp) can be used to map some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "import time\n",
    "\n",
    "def execute(context):\n",
    "\n",
    "    # Unzip the file that includes the 3 files that we need to transform and merge\n",
    "    myFileUploadName = context[\"fileUploadName\"]\n",
    "    with ZipFile(myFileUploadName, 'r') as f:\n",
    "        f.extractall()\n",
    "\n",
    "    # The 3 files are now stored in the Process Mining Server, in a directory dedicated to the process app, where the python code is stored and executed\n",
    "    # as well as where the ZIP file loaded by the user is stored.\n",
    "    # Therefore, all the unzipped files are accessible in the current directory\n",
    "    # sleep a while until all files are extracted\n",
    "    time.sleep(5)\n",
    "\n",
    "    requisitions = pd.read_csv('./requisition.csv')\n",
    "    create_requisitions = requisitions.copy()\n",
    "    create_requisitions['activity'] = 'Requisition Created'\n",
    "    create_requisitions.rename(columns={'Create_Date': 'datetime', 'Create_User': 'user',\n",
    "                            'Create_Role': 'role', 'Create_Type': 'type', 'Create_Source': 'source'}, inplace=True)\n",
    "    create_requisitions.drop(['Release_DateTime', 'Release_User', 'Release_Role',\n",
    "                            'Release_Type', 'Release_Source'], axis=1, inplace=True)\n",
    "\n",
    "    release_requisitions = requisitions.copy()\n",
    "    release_requisitions['activity'] = 'Requisition Released'\n",
    "    release_requisitions.rename(columns={'Release_DateTime': 'datetime', 'Release_User': 'user',\n",
    "                        'Release_Role': 'role', 'Release_Type': 'type', 'Release_Source': 'source'}, inplace=True)\n",
    "    release_requisitions.drop(['Create_Date', 'Create_User', 'Create_Role',\n",
    "                            'Create_Type', 'Create_Source'], axis=1, inplace=True)\n",
    "    # If some requisition cases are not complete, we remove rows where the requisition release date is NaN, as in\n",
    "    # this case, the activity did not yet occur. This is actually never the case.\n",
    "    release_requisitions = release_requisitions[release_requisitions['datetime'].notna()]\n",
    "\n",
    "    # procurements\n",
    "    procurements = pd.read_csv('./procurement.csv', low_memory=False)\n",
    "\n",
    "    # invoices\n",
    "    invoices = pd.read_csv('./invoice.csv')\n",
    "\n",
    "    # Merging invoice.csv information into procurement.csv\n",
    "    procurements = procurements.merge(invoices, on=\"Invoice_ID\", how=\"left\")\n",
    "\n",
    "    # Finally we append the requisition and the procurement event logs to create the final event log. Again, we can remove the events with a null `datetime`P2P_events = pd.concat([P2P_events, procurement_events])\n",
    "    P2P_events = pd.concat([create_requisitions, release_requisitions, procurements])\n",
    "    # removing rows with no datetime if any\n",
    "    P2P_events = P2P_events[P2P_events[\"datetime\"].notna()]\n",
    "    P2P_events = P2P_events.convert_dtypes()  # applying the best known types\n",
    "    # Reordering columns to simplify mapping\n",
    "    P2P_events = P2P_events[['activity','datetime', 'user', 'role', 'type',\n",
    "       'source',  'Req_ID','Req_Header', 'Req_Line', 'PO_Header', 'PO_Line', 'PO_ID', 'MatDoc_Header',\n",
    "       'MatDoc_Line', 'MatDoc_Year', 'MatDoc_ID', 'gr_h_y', 'Invoice_ID',\n",
    "       'rses_h', 'rses_l', 'rses_y', 'mandt', 'bukrs', 'xblnr', 'fl_h', 'fl_y',\n",
    "       'value_old', 'value_new', 'clear_doc', 'qmnum', 'data_gr_effettiva',\n",
    "       'usertype', 'Order_Type', 'Purchasing_Group', 'Purch_Group_Type',\n",
    "       'Material_Group_Area', 'Accounting_Type', 'Order_Vendor',\n",
    "       'Order_Source', 'Department', 'Order_Amount', 'Material',\n",
    "       'lead_time_material', 'Material_Type', 'Purch_Group_Area',\n",
    "       'Requisition_Plant', 'Order_Plant', 'Material_Plant', 'data_gr_ordine',\n",
    "       'data_gr_stat', 'data_gr_ipo', 'Paid_Amount', 'Paid_Vendor',\n",
    "       'split_ordine', 'split_riga_ordine', 'missmatch_riga_oda',\n",
    "       'check_riga_gagm', 'consegna_ipotetica', 'consegna_oda_ipotetica',\n",
    "       '_consegna_statistica_ipotetica_', 'pay_delay', 'pay_type',\n",
    "       'Req_Required_Vendor', 'Material_Group', 'Invoice_Date',\n",
    "       'Requisition_Vendor', 'Purchase_Organization', 'insert_date',\n",
    "       'Invoice_Header', 'Invoice_Year', 'Invoice_Amount', 'Invoice_Vendor',\n",
    "       'Invoice_Due_Date', 'Invoice_Vendor_City']]\n",
    "\n",
    "    return(P2P_events)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    context = {'fileUploadName':'P2P.zip'}\n",
    "    df = execute(context)\n",
    "    df.to_csv('P2Peventlog.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
