{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jira Ticketing Process App - Tutorial\n",
    "\n",
    "This process app requires a Jira account.\n",
    "\n",
    "We are going to use the following packages:\n",
    "- request to query data using Jira REST API\n",
    "- pandas to manipulate data frames -- tables to build the event log\n",
    "- json to manipulate query configurations\n",
    "- datetime to maniputate and transform dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Process App Python Code\n",
    "Upon start, the empty python code looks like this:\n",
    "- The execute() function the function called by IBM Process Mining when the process app is executed.\n",
    "- The main() function is not used by IBM Process Mining, but is very useful to develop and debug the process app as a standalone python program. Or when you want to solely execute the extractor from any machine, to generate an event log as a CSV file. \n",
    "\n",
    "The execute() function is called by IBM Process Mining with context, a JSON dictionary. \n",
    "- config: passes the user inputs of the process app\n",
    "- fileUploadName: refers to the ZIP file name that can be uploaded by the user, when the data source is passed as a series of files compressed in this ZIP file.\n",
    "\n",
    "In this short example, retrieve the user inputs of the process app, and we create an array of 4 events that is used to create the data frame that the execute() function returns. \n",
    "The user inputs are defined in the process app builder, and can be either mandatory or optional.\n",
    "\n",
    "The main() function creates the context that is normally passed by IBM Process Mining, run the execute() function, and generate a CSV file from the dataframe returned by the execute() function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def execute(context):\n",
    "    my_config = context['config']\n",
    "    url = my_config['url']\n",
    "    account = my_config['account']\n",
    "\n",
    "    aFewEvents = [\n",
    "        {'processid':'p1', 'activity':'analyze request', 'startdate':'2023-01-01'},\n",
    "        {'processid':'p1', 'activity':'approve request', 'startdate':'2023-01-02'},\n",
    "        {'processid':'p2', 'activity':'approve request', 'startdate':'2023-01-02'},\n",
    "        {'processid':'p2', 'activity':'reject request', 'startdate':'2023-01-04'},\n",
    "    ]\n",
    "    df = pd.DataFrame(aFewEvents)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    context = {'config': {'url':'https://aURL.com', 'account':'myaccount'}}\n",
    "    df = execute(context)\n",
    "    df.to_csv('eventlog.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jira Issue Table and Change History Table\n",
    "When creating a process app, we need to understand the location and the schema of the original data that we want to transform.\n",
    "\n",
    "Jira tickets are called issues.\n",
    "\n",
    "JIRA issues are stored in a JIRA table that can be accessed through a JIRA REST API.\n",
    "Each issue is composed of many common data like creation date, creator, project, etc. It also includes a long series of custom fields that each company can use as desired.\n",
    "Since this connector is generic, we will ignore the custom fields, but they could be easily added.\n",
    "\n",
    "For more information about the issue table: https://developer.atlassian.com/server/jira/platform/database-issue-fields/\n",
    "In this connector, we are not going to directly query the table. We will get the data via REST APIs.\n",
    "\n",
    "The issue table is not enough to recreate the issue lifecycle. We need to access the changelog table that keeps track of all the changes. All the issue fields can be changed, and the former and new value are kept in the change history table.\n",
    "For process mining, we considere that the interesting changes are when the issue status changes, and when an assignee changes. We could track other changes like priority changes, etc.\n",
    "For more information about the change history table: https://developer.atlassian.com/server/jira/platform/database-change-history/\n",
    "\n",
    "## Jira REST APIs, Accounts and API Tokens\n",
    "For more documentation about Jira REST APIs : https://developer.atlassian.com/server/jira/platform/rest-apis/\n",
    "\n",
    "Dependending on the Jira version you will be connected to, you can use v2 or v3. The APIs used in this connector are identical. https://developer.atlassian.com/cloud/jira/platform/rest/v3/intro/#about\n",
    "\n",
    "You need a Jira account, and an API token to call the Jira REST API. Follow the instructions from this documentation:\n",
    "https://support.atlassian.com/atlassian-account/docs/manage-api-tokens-for-your-atlassian-account/\n",
    "\n",
    "To execute this tutorial, copy [./my_config_template.json](./my_config_template.json) as ./my_config.json, and replace <YOUR JIRA>, <YOUR ACCOUNT>, and <YOUR TOKEN> with your values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Getting Issues with Jira REST APIs\n",
    "### Jira Projects and Issues\n",
    "An issue belongs to a project. We need to specify the projects from which we want to retrieve the issues, or we can retrieve the issues from all the projects.\n",
    "\n",
    "When we mine issues from all the projects, we can discard projects that do not have enough issues, that could be testing or experimental projects. \n",
    "\n",
    "The code below requests the Jira projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "\n",
    "def ws_get_projects(config):\n",
    "\n",
    "    auth = HTTPBasicAuth(config['user'],config['token'])\n",
    "\n",
    "    headers = {\n",
    "    \"Accept\": \"application/json\"\n",
    "    }\n",
    "    url = config['url']+'project'\n",
    "    response = requests.request(\n",
    "        \"GET\",\n",
    "        url,\n",
    "        headers=headers,\n",
    "        auth=auth)\n",
    "    if response.status_code == 200:\n",
    "        return json.loads(response.text)\n",
    "    else: \n",
    "        print(\"error get project %s \" % response.status_code)\n",
    "        return {'issues':[]}\n",
    "\n",
    "# Test the function\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # load the 'my_config.json' that includes your configuration\n",
    "    try: \n",
    "        f = open('./my_config.json')\n",
    "        my_config = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(\"*** WARNING: %s. Create ./my_config.json file from ./my_config_template.json\" % e)\n",
    "\n",
    "projects = ws_get_projects(my_config)\n",
    "print(\" %s projects found\" % len(projects))\n",
    "for project in projects:\n",
    "    print(project['key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the number of issues for each project\n",
    "For each project, we would like to count the number of issues. As mentioned above, we want to discard the projects that have too few issues, that could be testing or experimental projects.\n",
    "\n",
    "We also want the process app to be able to scope the issues to an optional time period. The time period is set with two user inputs: from_date and to_date.\n",
    "\n",
    "ws_count_tickets applies some tricks: this REST API call can return all the issues from a project. At that stage, we don't need to receive all the issues and their details because we will discard some projects.\n",
    "\n",
    "Jira supports paging, which means that we can specify the number of issues we want to retrieve, and the API returns the total number of issues that the project contains. This way, we can retrieve for example 100 issues, and call again the same API starting at issue 100, etc until we have retrieved all the issues.\n",
    "- mawResults : number of issues retrieved\n",
    "- startAt: index of the issue for which we start the retrieval\n",
    "- total: field returned that contains the total number of issues.\n",
    "\n",
    "In ws_count_tickets, we just want to get 1 issue per project (maxResults=1), and we check the total field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to count the number of issues per project\n",
    "def ws_count_tickets(config):\n",
    "    auth = HTTPBasicAuth(config['user'],config['token'])\n",
    "\n",
    "    query = 'project = ' + config['project_key']\n",
    "    if 'from_date' in config:\n",
    "        query = query + ' AND created >= ' + config['from_date']\n",
    "    if 'to_date' in config:\n",
    "        query = query + ' AND created <= ' + config['to_date']\n",
    "\n",
    "    params = {\n",
    "    'jql': query,\n",
    "    'fields':'created',\n",
    "    'maxResults' : 1,\n",
    "    'startAt': 0\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "    \"Accept\": \"application/json\"\n",
    "    }\n",
    "    url = config['url']+'search'\n",
    "    response = requests.request(\n",
    "        \"GET\",\n",
    "        url,\n",
    "        headers=headers,\n",
    "        params=params,\n",
    "        auth=auth)\n",
    "    if response.status_code == 200:\n",
    "        return json.loads(response.text)\n",
    "    else: \n",
    "        print(\"error count ticket %s \" % response.status_code)\n",
    "        return None\n",
    "    \n",
    "# Test the function\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # load the 'my_config.json' that includes your configuration\n",
    "    try: \n",
    "        f = open('./my_config.json')\n",
    "        my_config = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(\"*** WARNING: %s. Create ./my_config.json file from ./my_config_template.json\" % e)\n",
    "\n",
    "projects = ws_get_projects(my_config)\n",
    "print(\" %s projects found\" % len(projects))\n",
    "for project in projects:\n",
    "    my_config['project_key']=project['key']\n",
    "    result = ws_count_tickets(my_config)\n",
    "    print('project: %s : %s issues' % (project['key'], result['total']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the issues\n",
    "We need a function that returns the ticket details that we need, and the history of the ticket.\n",
    "\n",
    "The REST API that can query the issues of a project by using sql.\n",
    "\n",
    "Note that we do not need to make another query for the historical changes, this API enables getting the changelog of each issue from the same call.\n",
    "\n",
    "We don't need all the fields potentially available for an issue, as there are a lot of custom fields or fields that are not interesting for process mining. The standard_fields variable lists the fields that we want to retrieve, feel free to add or remove some.\n",
    "\n",
    "Finally, we need to use the paging capability. Our configuration file limits the number of issues retrieved to 100. We will thus loop again and again until we have retrieved all the issues from the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard fields that can be used in all JIRA/environements\n",
    "standard_fields = 'created,creator,project,resolution,resolutiondate,updated,duedate,timespent,timeestimate,timeoriginalestimate,status,issuetype,reporter,priority,assignee'\n",
    "\n",
    "\n",
    "def ws_get_tickets(config):\n",
    "    auth = HTTPBasicAuth(config['user'],config['token'])\n",
    "\n",
    "    query = 'project = ' + config['project_key']\n",
    "    if 'from_date' in config:\n",
    "        query = query + ' AND created >= ' + config['from_date']\n",
    "    if 'to_date' in config:\n",
    "        query = query + ' AND created <= ' + config['to_date']\n",
    "    params = {\n",
    "    'jql': query,\n",
    "    'fields': standard_fields,\n",
    "    'expand' : 'changelog',\n",
    "    'maxResults' : config['maxResults'],\n",
    "    'startAt': config['startAt']\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "    \"Accept\": \"application/json\"\n",
    "    }\n",
    "    url = config['url']+'search'\n",
    "    response = requests.request(\n",
    "        \"GET\",\n",
    "        url,\n",
    "        headers=headers,\n",
    "        params=params,\n",
    "        auth=auth)\n",
    "    if response.status_code == 200:\n",
    "        return json.loads(response.text)\n",
    "    else: \n",
    "        print(\"error get ticket %s \" % response.status_code)\n",
    "        return None\n",
    "    \n",
    "# Test the function\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # load the 'my_config.json' that includes your configuration\n",
    "    try: \n",
    "        f = open('./my_config.json')\n",
    "        my_config = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(\"*** WARNING: %s. Create ./my_config.json file from ./my_config_template.json\" % e)\n",
    "\n",
    "projects = ws_get_projects(my_config)\n",
    "print(\" %s projects found\" % len(projects))\n",
    "# Test the function with the first project\n",
    "# For this test, we limit the number of issues to 3 and we won't loop to get all of them\n",
    "project = projects[0]\n",
    "my_config['maxResults'] = 3\n",
    "my_config['project_key']=project['key']\n",
    "result = ws_get_tickets(my_config)\n",
    "print('project: %s : %s issues' % (project['key'], result['total']))\n",
    "result['issues']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Jira data into a Process Mining event log\n",
    "We have seen the main Jira REST API calls that we are going to use in this connector.\n",
    "\n",
    "Let's now focus on what we are doing with the data returned from Jira.\n",
    "\n",
    "### Selecting Process Mining relevant data for each ticket\n",
    "We need a function that extracts the data we want from an issue, and that returns a JSON object for each ticket.\n",
    "\n",
    "The JSON object ticket list will be used to create a Pandas dataframe (a table), that we will use to create our event log.\n",
    "\n",
    "Each issue is identified with a key that we keep as a ticket_id.\n",
    "\n",
    "The issue fields contains the data we need. Fields can be values (ex: issue['created'] =  creation date), or can be JSON objects (ex: issue['type'], from which we only need the name issue['type']['name'] ).\n",
    "Some fields might be empty, we need to check before addressing their keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ticket(issue):\n",
    "    ticket = { }\n",
    "    ticket['ticket_id'] = issue['key']\n",
    "    \n",
    "    issue_fields = issue['fields']\n",
    "    ticket['created'] = issue_fields['created']\n",
    "    # We could anonymize the name (same for the changelog)\n",
    "    # use creator.displayName for full name or creator.key. creator.key often contains the full name too...\n",
    "    ticket['creator'] = issue_fields['creator']['displayName']\n",
    "    ticket['resolutiondate'] = issue_fields['resolutiondate']\n",
    "    ticket['duedate'] = issue_fields['duedate']  \n",
    "    ticket['timespent'] = issue_fields['timespent']\n",
    "    ticket['timeestimate'] = issue_fields['timeestimate']\n",
    "    ticket['timeoriginalestimate'] = issue_fields['timeoriginalestimate']\n",
    "    ticket['project_key'] = issue_fields['project']['key']\n",
    "\n",
    "\n",
    "\n",
    "    # Fields that are dicts. Pick a value in the dict\n",
    "    if 'issuetype' in issue_fields: \n",
    "        ticket['type'] = issue_fields['issuetype']['name']\n",
    "    else:\n",
    "        ticket['type'] = None\n",
    "    if issue_fields['resolution']:\n",
    "        ticket['resolution'] = issue_fields['resolution']['name']\n",
    "    else: ticket['resolution'] = None \n",
    "    if issue_fields['reporter']:\n",
    "        ticket['reporter'] = issue_fields['reporter']['displayName']\n",
    "    else: ticket['reporter'] = None \n",
    "    if issue_fields['priority']:\n",
    "        ticket['priority'] = issue_fields['priority']['name']\n",
    "    else: ticket['priority'] = None \n",
    "    if issue_fields['assignee']:\n",
    "        ticket['assignee'] = issue_fields['assignee']['displayName']\n",
    "    else: ticket['assignee'] = None \n",
    "\n",
    "    return(ticket)\n",
    "\n",
    "# Test the function\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # load the 'my_config.json' that includes your configuration\n",
    "    try: \n",
    "        f = open('./my_config.json')\n",
    "        my_config = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(\"*** WARNING: %s. Create ./my_config.json file from ./my_config_template.json\" % e)\n",
    "\n",
    "projects = ws_get_projects(my_config)\n",
    "print(\" %s projects found\" % len(projects))\n",
    "\n",
    "# Test the function with the first project\n",
    "# For this test, we limit the number of issues to 10 and we won't loop to get all of them\n",
    "# Change the project ID until you find a project with issues that contain a changelog\n",
    "project = projects[2]\n",
    "my_config['maxResults'] = 10\n",
    "my_config['project_key']=project['key']\n",
    "result = ws_get_tickets(my_config)\n",
    "\n",
    "print('project: %s : %s issues' % (project['key'], result['total']))\n",
    "issues = result['issues']\n",
    "# Store each ticket in the ticket_list\n",
    "all_tickets = []\n",
    "for issue in issues:\n",
    "    ticket = create_ticket(issue)\n",
    "    all_tickets.append(ticket)\n",
    "\n",
    "all_tickets_df = pd.DataFrame(all_tickets)\n",
    "all_tickets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the Change log\n",
    "Each issue returned by the REST API can contain a changelog.\n",
    "\n",
    "The changelog can be a complex array that contains several changes at several dates.\n",
    "\n",
    "We want to create a list of change logs that we index with the ticket_id.\n",
    "\n",
    "WARNING: some projects do not store the changelog. Try several projects until one returns non empty change logs\n",
    "\n",
    "Each change log contains a histories field that contains one or several changes (see maxResults and total).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # load the 'my_config.json' that includes your configuration\n",
    "    try: \n",
    "        f = open('./my_config.json')\n",
    "        my_config = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(\"*** WARNING: %s. Create ./my_config.json file from ./my_config_template.json\" % e)\n",
    "\n",
    "projects = ws_get_projects(my_config)\n",
    "print(\" %s projects found\" % len(projects))\n",
    "\n",
    "# Test the function with the first project\n",
    "# For this test, we limit the number of issues to 10 and we won't loop to get all of them\n",
    "\n",
    "# Change the project ID until you find a project with issues that contain a changelog\n",
    "project = projects[2]\n",
    "my_config['maxResults'] = 10\n",
    "my_config['project_key']=project['key']\n",
    "result = ws_get_tickets(my_config)\n",
    "\n",
    "print('project: %s : %s issues' % (project['key'], result['total']))\n",
    "issues = result['issues']\n",
    "\n",
    "# Store all the changelog in the changelog_list\n",
    "changelog_list = []\n",
    "for issue in issues:\n",
    "    changelog = issue['changelog']\n",
    "    changelog['ticket_id'] =  issue['key']\n",
    "    # We could add other case-level data\n",
    "    changelog_list.append(issue['changelog'])\n",
    "\n",
    "changelog_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the change log 2/2\n",
    "We process the changelog_list to extract each individual change log. \n",
    "Then we create a change log JSON object that we store in a list that will be use to create a Pandas dataframe (table).\n",
    "We don't need to keep all the changes, we only create a change log object when the status or the assignee is changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ticket_changelog(ticket_id, author, datestr, item):\n",
    "    # Only keep changes of status and assignee (you can change this)\n",
    "    field_changes_to_keep = ['status', 'assignee']\n",
    "    if item['field'] not in field_changes_to_keep:\n",
    "        return 0\n",
    "\n",
    "    ticket_change = {}\n",
    "    ticket_change['ticket_id'] = ticket_id\n",
    "    ticket_change['author'] = author\n",
    "    ticket_change['created'] = datestr\n",
    "    ticket_change['field'] =  item['field']\n",
    "    ticket_change['from'] = item['from']\n",
    "    ticket_change['fromString'] = item['fromString']\n",
    "    ticket_change['to'] = item['to']\n",
    "    ticket_change['toString'] = item['toString']\n",
    "    return ticket_change\n",
    "\n",
    "ticket_changes_list =  []\n",
    "for changelog in changelog_list:\n",
    "    # there could be several histories in each change log (histories==values when calling the {ticketid}/changelog API)\n",
    "    for history in changelog['histories']:\n",
    "        # there could be several item changes in each history\n",
    "        for item in history['items']:\n",
    "            ticket_changelog = create_ticket_changelog(changelog['ticket_id'], history['author']['key'], history['created'], item)\n",
    "            if ticket_changelog :\n",
    "                ticket_changes_list.append(ticket_changelog)\n",
    "\n",
    "print(\"Total number of changelogs: %s\" % len(ticket_changes_list))\n",
    "all_ticket_changes_df = pd.DataFrame(ticket_changes_list)\n",
    "all_ticket_changes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pandas to create an event log from the ticket and the change log data frames\n",
    "### Ticket Creation events\n",
    "The Ticket Created events are created from the tickets data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ticket creation events\n",
    "ticket_created_df = all_tickets_df.copy()\n",
    "# Add a field 'activity'\n",
    "ticket_created_df['activity'] = 'Ticket Created'\n",
    "# Replace the fieldname creator by user\n",
    "ticket_created_df.rename(columns={'creator':'user'}, inplace=True)\n",
    "# Add a field 'start_date' from the field 'created' (we could have renamed it)\n",
    "ticket_created_df['start_date'] = ticket_created_df['created']\n",
    "# Re-order the fields\n",
    "ticket_created_df = ticket_created_df[['ticket_id','activity','start_date','user', 'project_key','type', 'priority', 'resolutiondate', 'duedate',\n",
    "    'timespent', 'timeestimate', 'timeoriginalestimate', 'resolution', 'reporter', 'assignee']]\n",
    "print(\"Total number of tickets created: %s\" % len(ticket_created_df))\n",
    "ticket_created_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Status changes events\n",
    "For process mining we are interested in tracking all the ticket status changes. The changelog provides this information.\n",
    "Each time the field 'status' is changed, we create an activity named 'Ticket ' to which we happend the name of the new status. We also store the date and the author of the change "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an event log from the ticket status changes\n",
    "status_changes_df = all_ticket_changes_df[all_ticket_changes_df['field']=='status'].copy()\n",
    "status_changes_df['activity'] = 'Ticket ' + status_changes_df['toString']\n",
    "status_changes_df.rename(columns={'author':'user'}, inplace=True)\n",
    "status_changes_df['start_date'] = status_changes_df['created']\n",
    "status_changes_df = status_changes_df[['ticket_id', 'activity', 'start_date','user']]\n",
    "print(\"Total number of status changes in changelog: %s\" % len(status_changes_df))\n",
    "status_changes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignee changes events\n",
    "Changing the ticket assignee is potentially an important event for process mining, we create a table of such changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create event log from ticket assignee changes\n",
    "assignee_changes_df = all_ticket_changes_df[all_ticket_changes_df['field']=='assignee'].copy()\n",
    "assignee_changes_df['activity'] = 'Ticket Assigned'\n",
    "assignee_changes_df['start_date'] = assignee_changes_df['created']\n",
    "assignee_changes_df.rename(columns={'author':'user', 'toString':'assignee'}, inplace=True)\n",
    "assignee_changes_df = assignee_changes_df[['ticket_id', 'activity', 'start_date','user','assignee']]\n",
    "print(\"Total number of assignee changes in changelog: %s\" % len( assignee_changes_df))\n",
    "assignee_changes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Process Mining Event Log\n",
    "For this connector, we only need to concatenate the 3 dataframes to create our final event log.\n",
    "\n",
    "To simplify the mapping in IBM Process Mining, we reformat the Jira dates into ISO dates such that they are easily detected during the mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XformToIso(d):\n",
    "    if d == '':\n",
    "        return ''\n",
    "    else:\n",
    "        aDate = datetime.strptime(d,'%Y-%m-%dT%H:%M:%S.%f%z')\n",
    "    return  aDate.isoformat(sep = 'T', timespec = 'milliseconds')\n",
    "\n",
    "# CREATE THE FINAL EVENT LOG\n",
    "eventlog_df = pd.concat([ticket_created_df, assignee_changes_df, status_changes_df])\n",
    "# Replace NaN by blank ''\n",
    "eventlog_df.fillna('', inplace=True)\n",
    "# Change the date format such that it is automatically understood in Process Mining\n",
    "eventlog_df['start_date'] = eventlog_df['start_date'].apply(XformToIso)\n",
    "eventlog_df['resolutiondate'] = eventlog_df['resolutiondate'].apply(XformToIso)\n",
    "eventlog_df.fillna('', inplace=True)\n",
    "# force the type to strings\n",
    "eventlog_df.astype(str)\n",
    "eventlog_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we have covered the principal steps required to get data from a Jira server, to select the data we want to keep in process mining, to transform these data into IBM Process Mining compatible format, and to eventually generate the event log as a Pandas dataframe.\n",
    "\n",
    "The complete python program that you upload in the Process App is located [here](./JiraConnector.py)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
