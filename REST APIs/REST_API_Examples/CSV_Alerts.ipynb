{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import time\n",
    "import hashlib, hmac, base64\n",
    "import requests, json, urllib3\n",
    "\n",
    "requests.packages.urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "import sys\n",
    "\n",
    "SPINNING_RATE = 0.1\n",
    "TIME_ADJUST = 0\n",
    "PRINT_TRACE = 0\n",
    "\n",
    "# config is passed to most function, this is a json object that includes all the required parameters\n",
    "\n",
    "def spinning_cursor():\n",
    "    while True:\n",
    "        for cursor in '|/-\\\\':\n",
    "            yield cursor\n",
    "\n",
    "def init_params_headers(config, headers, params):\n",
    "    params.update({'org' : config['org_key']})     \n",
    "    headers.update({\"content-type\": \"application/json\", \"Authorization\": \"Bearer %s\" % config['sign'] })\n",
    "\n",
    "\n",
    "def getTimestamp():\n",
    "    return str(int(time.time() * 1000) + TIME_ADJUST) ;\n",
    "\n",
    "def hmacSHA256(api_key, values):\n",
    "    message = bytes(\"\".join(values),encoding='utf8')\n",
    "    return base64.b64encode(hmac.new( bytes(api_key, encoding='utf8'), message, digestmod=hashlib.sha256).digest()).decode(\"utf-8\")\n",
    "\n",
    "def ws_post_sign(config):\n",
    "\n",
    "    url = \"%s/integration/sign\" % config['url']\n",
    "    headers = {\"content-type\": \"application/json\"}\n",
    "    data = { 'uid' : config['user_id'], 'apiKey' : config['api_key']}\n",
    "    if( PRINT_TRACE) : print(\"REST CALL: \"+url+\" \"+str(data))\n",
    "    r = requests.post(url, verify=False, data=json.dumps(data), headers=headers)\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Error: get signature: %s\" % r.json()['data'])\n",
    "        config['sign'] = 0\n",
    "        return 0\n",
    "    else:\n",
    "        config['sign'] = r.json()['sign']\n",
    "    return r.json()['sign']\n",
    "\n",
    "\n",
    "def ws_delete_process(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/processes/%s\" % (config['url'], config['project_key'])\n",
    "\n",
    "    if( PRINT_TRACE) : print(\"REST CALL: \"+ url+\" \"+str(params))\n",
    "    r = requests.delete(url, verify=False, params=params, headers=headers)\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Error: delete process: %s\" % r.json()['data'])        \n",
    "    return r\n",
    "\n",
    "def ws_proc_post(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/processes\" % (config['url'])\n",
    "    data = { 'title' : config['project_name'], 'org' : config['org_key']}\n",
    "    if( PRINT_TRACE) : print(\"REST CALL: \"+ url+\" \"+str(data))\n",
    "    r = requests.post(url, verify=False, data=json.dumps(data), headers=headers, params=params)\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Error: Process %s creation: %s\" % (config['project_name'], r.json()['data']))        \n",
    "    return r\n",
    "\n",
    "def ws_csv_upload(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "    # no content-type \n",
    "    headers.pop(\"content-type\")\n",
    "\n",
    "    url = \"%s/integration/csv/%s/upload\" % (config['url'], config['project_key'])\n",
    "    if( PRINT_TRACE) : print(\"REST CALL: \"+ url+\" \"+str(params))\n",
    "    files = {'file': (config['csv_filename'], open(config['csv_filename'], 'rb'),'text/zip')}\n",
    "    r = requests.post(url, verify=False, files=files, params=params, headers=headers)\n",
    "    # Async call --- the caller need to loop on get job status that it is completed\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Error: CSV upload: %s\" % r.json()['data'])        \n",
    "    return r\n",
    "\n",
    "\n",
    "def ws_query_post(config, query):\n",
    "    headers = {}\n",
    "    params = {}\n",
    "    init_params_headers(config, headers, params)\n",
    "    headers['content-type'] = 'application/x-www-form-urlencoded'\n",
    "    url = \"%s/analytics/integration/%s/query\" % (\n",
    "        config['url'], config['project_key'])\n",
    "    data = \"params={'query': '%s'}\" % query\n",
    "    print(data)\n",
    "    r = requests.post(url, verify=False, params=params,\n",
    "                      headers=headers, data=data)\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Error: query post: %s\" % r.json()['data'])\n",
    "    return r\n",
    "\n",
    "\n",
    "def ws_backup_upload(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "    # no content-type \n",
    "    headers.pop(\"content-type\")\n",
    "\n",
    "    url = \"%s/integration/processes/%s/upload-backup\" % (config['url'], config['project_key'])\n",
    "\n",
    "    if( PRINT_TRACE) : print(\"REST CALL: \"+ url+\" \"+str(params))\n",
    "    files = {'file': (config['backup_filename'], open(config['backup_filename'], 'rb'),'text/zip')}\n",
    "    r = requests.post(url, verify=False, files=files, params=params, headers=headers)\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Error: backup upload: %s\" % r.json()['data']) \n",
    "    return r\n",
    "\n",
    "def ws_get_backup_list(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url  = \"%s/integration/processes/%s/backups\" % (config['url'], config['project_key'])\n",
    "    if( PRINT_TRACE) : print(\"REST CALL: \"+ url+\" \"+str(params))\n",
    "    r = requests.get(url, verify=False, headers=headers, params=params)\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Error: backup list get: %s\" % r.json()['data'])\n",
    "    return r\n",
    "\n",
    "def getBackupIdByMessage(backuplist, message):\n",
    "    backuplist = backuplist['backups']\n",
    "    for backup in backuplist:\n",
    "        if (backup['message'] == message) :\n",
    "            return backup['id']\n",
    "    return 0\n",
    "\n",
    "def ws_apply_backup(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/processes/%s/backups/%s\" % (config['url'], config['project_key'], config['backup_id'])\n",
    "    if( PRINT_TRACE) : print(\"REST CALL: \"+ url+\" \"+str(params))\n",
    "    r = requests.put(url, verify=False, headers=headers, params=params)\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Error: apply backup: %s : %s\" % (r.json()['data'], config['backup_id'])) \n",
    "    return r\n",
    "\n",
    "def ws_create_log(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/csv/%s/create-log\" % (config['url'], config['project_key'])\n",
    "    if( PRINT_TRACE) : print(\"REST CALL: \"+ url+\" \"+str(params))\n",
    "    r = requests.post(url, verify=False, headers=headers, params=params)\n",
    "    # Async function, the caller needs to check the job status (returned in r)\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Error: create log: %s\" % r.json()['data']) \n",
    "    return r\n",
    "\n",
    "def ws_get_csv_job_status(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/csv/job-status/%s\" % (config['url'], config['job_key'])\n",
    "    if( PRINT_TRACE) : print(\"REST CALL: \"+ url+\" \"+str(params))\n",
    "    r = requests.get(url, verify=False, headers=headers, params=params)\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Error: get CSV job status: %s\" % r.json()['data'])        \n",
    "    return r\n",
    "\n",
    "def create_and_load_new_project(config):\n",
    "    print(\"Process Mining: creating new project\")\n",
    "    r_proc = ws_proc_post(config)\n",
    "    if (r_proc.status_code != 200): return r_proc\n",
    "    config['project_key']= r_proc.json()['projectKey']\n",
    "    print(\"Process Mining: loading event log (please wait)\")\n",
    "    r = ws_csv_upload(config)\n",
    "    if (r.status_code != 200): return r\n",
    "    else :\n",
    "        # wait until async call is completed\n",
    "        config['job_key']= r.json()['data']\n",
    "        runningCall = 1\n",
    "        spinner = spinning_cursor()\n",
    "        while  runningCall :\n",
    "            r = ws_get_csv_job_status(config)\n",
    "            if (r.json()['data'] == 'complete') :\n",
    "                runningCall = 0\n",
    "            if (r.json()['data'] == 'error') :\n",
    "                runningCall = 0\n",
    "                print(\"Error while loading CSV -- column number mismatch\")\n",
    "                return 0\n",
    "            sys.stdout.write(next(spinner))\n",
    "            sys.stdout.flush()\n",
    "            sys.stdout.write('\\b')\n",
    "            time.sleep(SPINNING_RATE)\n",
    "\n",
    "    r = ws_backup_upload(config)\n",
    "    if (r.status_code != 200): return r\n",
    "    config['backup_id'] = r.json()['backupInfo']['id']\n",
    "    r = ws_apply_backup(config)\n",
    "    if (r.status_code != 200): return r\n",
    "    print(\"Process Mining: refreshing process (please wait)\")\n",
    "    r = ws_create_log(config)\n",
    "    if (r.status_code != 200): return r\n",
    "    else :\n",
    "        # wait until async call is completed\n",
    "        config['job_key']= r.json()['data']\n",
    "        runningCall = 1\n",
    "        #pbar = tqdm(total=100)\n",
    "        spinner = spinning_cursor()\n",
    "        while  runningCall :\n",
    "            r = ws_get_csv_job_status(config)\n",
    "            if (r.json()['data'] == 'complete') :\n",
    "                runningCall = 0\n",
    "            if (r.json()['data'] == 'error') :\n",
    "                runningCall = 0\n",
    "                print(\"Error while creating the log\")\n",
    "                return 0\n",
    "            sys.stdout.write(next(spinner))\n",
    "            sys.stdout.flush()\n",
    "            sys.stdout.write('\\b')\n",
    "            time.sleep(SPINNING_RATE)\n",
    "    return r_proc\n",
    "\n",
    "def upload_csv_and_createlog(config) :\n",
    "    r = ws_csv_upload(config)\n",
    "    if (r.status_code != 200): return r\n",
    "    else :\n",
    "        # wait until async call is completed\n",
    "        config['job_key']= r.json()['data']\n",
    "        runningCall = 1\n",
    "        print(\"Process Mining: loading event log (please wait)\")\n",
    "        spinner = spinning_cursor()\n",
    "        while  runningCall :\n",
    "            r = ws_get_csv_job_status(config)\n",
    "            if (r.json()['data'] == 'complete') :\n",
    "                runningCall = 0\n",
    "            if (r.json()['data'] == 'error') :\n",
    "                runningCall = 0\n",
    "                print(\"Error while loading CSV -- column number mismatch\")\n",
    "                return 0\n",
    "            sys.stdout.write(next(spinner))\n",
    "            sys.stdout.flush()\n",
    "            sys.stdout.write('\\b')\n",
    "            time.sleep(SPINNING_RATE)\n",
    "\n",
    "    r = ws_create_log(config)\n",
    "    if (r.status_code != 200): return r\n",
    "    else :\n",
    "        # wait until async call is completed\n",
    "        config['job_key']= r.json()['data']\n",
    "        runningCall = 1\n",
    "        print(\"Process Mining: refreshing model (please wait)\")\n",
    "        spinner = spinning_cursor()\n",
    "        while  runningCall :\n",
    "            r = ws_get_csv_job_status(config)\n",
    "            if (r.json()['data'] == 'complete') :\n",
    "                runningCall = 0\n",
    "            if (r.json()['data'] == 'error') :\n",
    "                runningCall = 0\n",
    "                print(\"Error while creating the log\")\n",
    "                return 0\n",
    "            sys.stdout.write(next(spinner))\n",
    "            sys.stdout.flush()\n",
    "            sys.stdout.write('\\b')\n",
    "            time.sleep(SPINNING_RATE)\n",
    "    return r\n",
    "\n",
    "# List all the dashboards of a given process mining project\n",
    "def ws_get_dashboards(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/analytics/integration/dashboard/%s/list\" % (config['url'], config['project_key'])\n",
    "    r = requests.get(url, verify=False, params=params, headers=headers)\n",
    "    if (r.status_code == 200):\n",
    "        values = r.json()\n",
    "        return {'status_code': r.status_code, 'data' : values['data']['dashboards']}\n",
    "    else:\n",
    "        return {'status_code': r.status_code, 'data' : None}\n",
    "\n",
    "# Returns only the table widgets in the dashboard_name\n",
    "def ws_get_widgets(config):\n",
    "    res = ws_get_dashboards(config)\n",
    "\n",
    "    if (res['status_code'] == 200): dashboards = res['data']\n",
    "    else:\n",
    "        return {'status_code': res['status_code'], 'data': None}\n",
    "\n",
    "    for aDashboard in dashboards:\n",
    "        if (aDashboard['name'] == config['dashboard_name']):\n",
    "            dashboard = aDashboard\n",
    "\n",
    "    if dashboard == 0 :\n",
    "        print(\"ERROR: dashboard does not exist\")\n",
    "        return 0\n",
    "    dashboard_id = dashboard['id']\n",
    "\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/analytics/integration/dashboard/%s/%s/list\" % (config['url'], config['project_key'], dashboard_id)\n",
    "    r = requests.get(url, verify=False, params=params, headers=headers )\n",
    "    if (res['status_code'] == 200):\n",
    "        widgets = r.json() \n",
    "        return {'status_code': res['status_code'], 'data': widgets['data']['widgets']}\n",
    "    else:\n",
    "        return {'status_code': res['status_code'], 'data': None}\n",
    "\n",
    "def ws_get_widget_values(config):\n",
    "    dashboard_id = config.get('dashboard_id')\n",
    "    if dashboard_id is None:\n",
    "        # add the dashboard_id to the widget data\n",
    "        print(\"....searching dashboard_id\")\n",
    "        res = ws_get_dashboards(config)\n",
    "        if (res['status_code'] == 200): dashboards = res['data']\n",
    "        else:\n",
    "            return {'status_code': res['status_code'], 'data': None}\n",
    "        dashboard = 0\n",
    "        for aDashboard in dashboards:\n",
    "            if (aDashboard['name'] == config['dashboard_name']):\n",
    "                dashboard = aDashboard\n",
    "        if (dashboard == 0) :\n",
    "            print(\"ERROR: dashboard %s does not exist\" % config['dashboard_name'])\n",
    "            return {'status_code': 200, 'data': None}\n",
    "        else:\n",
    "            # Store the dashboard id in the widget to avoid calling again the rest API to retrieve the dashboard by name\n",
    "            config['dashboard_id'] = dashboard['id']\n",
    "\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/analytics/integration/dashboard/%s/%s/%s/retrieve\" % (config['url'],\n",
    "                                                                        config['project_key'],\n",
    "                                                                        config['dashboard_id'],\n",
    "                                                                        config['widget_id'])\n",
    "\n",
    "    r = requests.get(url, verify=False, params=params, headers=headers )\n",
    "    if (r.status_code == 200):\n",
    "        values = r.json()\n",
    "        return {'status_code': r.status_code, 'data': values['data']}\n",
    "    else:\n",
    "        return {'status_code': r.status_code, 'data': None}\n",
    "\n",
    "def ws_create_update_variables(config, variablesArray):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/processes/%s/variables\" % (config['url'], config['project_key'])\n",
    "    r = requests.post(url, verify=False, data=json.dumps(variablesArray), params=params, headers=headers )\n",
    "    return r\n",
    "\n",
    "def ws_get_variable(config, variablename):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "    \n",
    "    url = \"%s/integration/processes/%s/variables/%s\" % (config['url'], config['project_key'], variablename)\n",
    "    r = requests.get(url, verify=False, params=params, headers=headers )\n",
    "    if (r.status_code == 200):\n",
    "        values = r.json()\n",
    "        return values['data']\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def ws_get_variables(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/processes/%s/variables\" % (config['url'], config['project_key'])\n",
    "    r = requests.get(url, verify=False, params=params, headers=headers )\n",
    "    if (r.status_code == 200):\n",
    "        values = r.json()\n",
    "        return values['data']\n",
    "    else:\n",
    "        print(\"Error: ws_get_variables %s\" % r.json()['data'])\n",
    "        return []\n",
    "\n",
    "def ws_delete_variable(config, variablename):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/processes/%s/variables/%s\" % (config['url'], config['project_key'], variablename)\n",
    "    r = requests.delete(url, verify=False, params=params, headers=headers )\n",
    "    return r.status_code\n",
    "\n",
    "def ws_delete_variables(config):\n",
    "    variables = ws_get_variables(config)\n",
    "    for variable in variables:\n",
    "        ws_delete_variable(config, variable['name'])\n",
    "\n",
    "def ws_get_processes(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/processes\" % (config['url'])\n",
    "    r = requests.get(url, verify=False, params=params, headers=headers )\n",
    "    if (r.status_code == 200):\n",
    "        values = r.json()\n",
    "        return values['data']\n",
    "    else:\n",
    "        return r\n",
    "\n",
    "def ws_get_project_info(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/processes/%s\" % (config['url'], config['project_key'])\n",
    "\n",
    "    return(requests.get(url, verify=False, params=params, headers=headers ))\n",
    "\n",
    "\n",
    "def ws_get_project_meta_info(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/csv/%s/meta\" % (config['url'], config['project_key'])\n",
    "\n",
    "    r = requests.get(url, verify=False, params=params, headers=headers )\n",
    "    if (r.status_code == 200):\n",
    "        values = r.json()\n",
    "        return values['data']\n",
    "    else:\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime as dt, timedelta\n",
    "import sys\n",
    "import ProcessMining_API as IPM\n",
    "import pandas as pd\n",
    "\n",
    "def join_and_cleanup(df1, df2, cols, suffix, suffix_for):\n",
    "    # suffix_for = 'left' or 'right'\n",
    "    if (suffix_for == 'left'): suffixes = (suffix, '')\n",
    "    else: suffixes = ('', suffix)\n",
    "    df = df1.merge(df2, how='left', on=cols, indicator='alert_is_present_on', suffixes=suffixes)\n",
    "    # remove columns (duplicated) from log\n",
    "    kept_cols = []\n",
    "    for col in df.columns:\n",
    "        if (suffix not in col):\n",
    "            kept_cols.append(col)\n",
    "    return df[kept_cols]\n",
    "\n",
    "class processMiningConnector():\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def copy(self):\n",
    "        return processMiningConnector(self.config.copy())\n",
    "\n",
    "class widgetAlerts():\n",
    "    def __init__(self, connector, dashboardName, widgetId, matchingColumnIndexes): # config is a json with the widget properties\n",
    "        # create a copy of the connector because connector.config holds the dashboard_id when it is found the first time\n",
    "        self.connector = connector.copy()\n",
    "        self.dashboardName = dashboardName\n",
    "        self.widgetId = widgetId\n",
    "        self.matchingColumnIndexes = matchingColumnIndexes # replaced when the widget columns are known\n",
    "        if (self.matchingColumnIndexes == 0 or self.matchingColumnIndexes == ''):\n",
    "            self.matchingColumnIndexes = [0]\n",
    "        self.logFileName = 'alert_log_' + self.dashboardName + '_' + self.widgetId +'.csv'\n",
    "        self.summaryFileName = 'alert_summary_' + self.dashboardName + '_' + self.widgetId +'.csv'\n",
    "        self.connector.config['dashboard_name'] = dashboardName\n",
    "        self.connector.config['widget_id'] = widgetId\n",
    "\n",
    "\n",
    "    def loadLog(self):\n",
    "        # Load alert log for this widget\n",
    "        try: \n",
    "            self.log_df = pd.read_csv(self.logFileName, dtype=str)\n",
    "        except:\n",
    "            # First call\n",
    "            self.log_df = pd.DataFrame()\n",
    "\n",
    "    def loadSummary(self):\n",
    "        # Load summary file for this widget\n",
    "        try: \n",
    "            self.summary_df = pd.read_csv(self.summaryFileName)\n",
    "        except:\n",
    "            # First call: Create the summary df \n",
    "            self.summary_df = pd.DataFrame()\n",
    "\n",
    "    def loadWidgetData(self):\n",
    "        ws_post_sign(self.connector.config)\n",
    "        res = ws_get_widget_values(self.connector.config)\n",
    "        if (res['status_code'] == 200):\n",
    "            self.widget_df = pd.DataFrame(res['data'])\n",
    "            self.setMatchingColumns() # matching columns: indexes replaced with column names\n",
    "\n",
    "        else: \n",
    "            print('ERROR: Dashboard or Widget not found')\n",
    "            self.widget_df = pd.DataFrame()\n",
    "\n",
    "    def setMatchingColumns(self):\n",
    "        # Get the widget columns used to match the alerts\n",
    "        # Do the merge with the columns mentionned in the configuration as an array\n",
    "        self.matchingColumns = []\n",
    "        for i in range(len(self.widget_df.columns)):\n",
    "            if (i in self.matchingColumnIndexes):\n",
    "                self.matchingColumns.append(self.widget_df.columns[i])\n",
    "        print(\"Matching columns used for existing alerts: %s\" % self.matchingColumns)\n",
    "    \n",
    "    def updateAlertsFirstTime(self):\n",
    "        if len(self.widget_df) == 0:\n",
    "            print('Empty widget and no log yet. No data to generate')\n",
    "            self.new_log_df = pd.DataFrame()\n",
    "            self.summary_df = pd.DataFrame()\n",
    "        else: #first time execution\n",
    "            self.new_log_df = self.widget_df.copy()\n",
    "            self.new_log_df['alert_status'] = 'NEW'\n",
    "            self.new_log_df['alert_creation_date'] = dt.now().isoformat()\n",
    "            self.new_log_df['alert_closed_date'] = ''\n",
    "            self.summary_df = pd.DataFrame([{\n",
    "                'update_date': dt.now().isoformat(),\n",
    "                'new': len(self.widget_df),\n",
    "                'pending': 0,\n",
    "                'closed': 0,\n",
    "                'new_to_pending': 0,\n",
    "                'new_to_closed': 0,\n",
    "                'pending_to_closed': 0,\n",
    "                'pending_to_pending': 0,\n",
    "                'any_to_closed': 0,\n",
    "                'progression_rate': 0\n",
    "            }])\n",
    "\n",
    "    def updateAlerts(self):\n",
    "        self.loadLog()\n",
    "        self.loadSummary()\n",
    "        self.loadWidgetData()\n",
    "\n",
    "        if len(self.log_df) == 0: # No log yet\n",
    "            self.updateAlertsFirstTime()\n",
    "        else:\n",
    "            self.new_log_df = self.log_df[self.log_df.alert_status == 'CLOSED'] # we don't touch CLOSED alerts\n",
    "            # Add a new summary row\n",
    "            self.summary_df.loc[len(self.summary_df)] = { \n",
    "                'update_date': dt.now().isoformat(),\n",
    "                'new': 0,\n",
    "                'pending': 0,\n",
    "                'closed': self.summary_df.loc[len(self.summary_df)-1, 'closed'],\n",
    "                'new_to_pending': 0,\n",
    "                'new_to_closed': 0,\n",
    "                'pending_to_closed': 0,\n",
    "                'pending_to_pending': 0,\n",
    "                'any_to_closed': 0,\n",
    "                'progression_rate': 0\n",
    "            }\n",
    "            if len(self.widget_df) == 0: # no more alerts, close all NEW and PENDING from log\n",
    "                self.closeAllAlerts()\n",
    "            else:\n",
    "                self.manageNewAlerts()\n",
    "                self.manageExistingAlerts()\n",
    "        \n",
    "        self.log_df = self.new_log_df\n",
    "\n",
    "    def closeAllAlerts(self):\n",
    "        new_to_close_df = self.log_df[self.log_df.alert_status == 'NEW']\n",
    "        new_to_close_df.alert_status = 'CLOSED'\n",
    "        new_to_close_df.alert_closed_date = dt.now().isoformat()\n",
    "        pending_to_close_df = self.log_df[self.log_df.alert_status == 'PENDING']\n",
    "        pending_to_close_df.alert_status = 'CLOSED'\n",
    "        pending_to_close_df.alert_closed_date = dt.now().isoformat()\n",
    "        self.new_log_df = pd.concat([self.new_log_df, new_to_close_df, pending_to_close_df])\n",
    "        self.summary_df.loc[len(self.summary_df) - 1, 'closed'] = len(self.log_df)\n",
    "        self.summary_df.loc[len(self.summary_df) - 1, 'new_to_closed'] = len(new_to_close_df)\n",
    "        self.summary_df.loc[len(self.summary_df) - 1, 'pending_to_closed'] = len(pending_to_close_df)\n",
    "\n",
    "    def manageNewAlerts(self):\n",
    "        not_closed_df = self.log_df[self.log_df.alert_status != 'CLOSED']\n",
    "        widget_new_df = join_and_cleanup(self.widget_df, not_closed_df, self.matchingColumns, \"_suffixFromlog\", 'right')\n",
    "        # Alerts in the widget with 'exist'==left_only are NEW\n",
    "        widget_new_df = widget_new_df[widget_new_df.alert_is_present_on == 'left_only']\n",
    "        widget_new_df.alert_status = 'NEW'\n",
    "        widget_new_df.alert_creation_date = dt.now().isoformat()\n",
    "        widget_new_df.alert_closed_date = ''\n",
    "        self.new_log_df = pd.concat([self.new_log_df, widget_new_df.drop(columns=['alert_is_present_on'])])\n",
    "        self.summary_df.loc[len(self.summary_df) - 1, 'new'] = len(widget_new_df)\n",
    "\n",
    "    def manageExistingAlerts(self):\n",
    "        not_closed_df = self.log_df[self.log_df.alert_status != 'CLOSED']\n",
    "        not_closed_df = join_and_cleanup(not_closed_df, self.widget_df, self.matchingColumns, \"_suffixFromlog\", 'left')\n",
    "\n",
    "        # NEW to PENDING  \n",
    "        new_to_pending_df = not_closed_df.query('alert_status==\"NEW\" & alert_is_present_on==\"both\"')\n",
    "        self.summary_df.loc[len(self.summary_df) - 1, 'new_to_pending'] = len(new_to_pending_df)\n",
    "        self.summary_df.loc[len(self.summary_df) - 1, 'pending'] = len(new_to_pending_df)\n",
    "        if (len(new_to_pending_df)):\n",
    "            new_to_pending_df.alert_status = 'PENDING'\n",
    "            self.new_log_df = pd.concat([self.new_log_df, new_to_pending_df.drop(columns=['alert_is_present_on'])])\n",
    "\n",
    "        # NEW to CLOSED\n",
    "        new_to_close_df = not_closed_df.query('alert_status==\"NEW\" & alert_is_present_on==\"left_only\"')\n",
    "        self.summary_df.loc[len(self.summary_df) - 1, 'new_to_closed'] = len(new_to_close_df)\n",
    "        self.summary_df.loc[len(self.summary_df) - 1, 'closed'] += len(new_to_close_df)\n",
    "        if (len(new_to_close_df)):\n",
    "            new_to_close_df.alert_status = 'CLOSED'\n",
    "            new_to_close_df.alert_closed_date = dt.now().isoformat()\n",
    "            self.new_log_df = pd.concat([self.new_log_df, new_to_close_df.drop(columns=['alert_is_present_on'])])\n",
    "\n",
    "        # PENDING to PENDING\n",
    "        pending_to_pending_df = not_closed_df.query('alert_status==\"PENDING\" & alert_is_present_on==\"both\"')\n",
    "        self.summary_df.loc[len(self.summary_df) - 1, 'pending_to_pending'] = len(pending_to_pending_df)\n",
    "        self.summary_df.loc[len(self.summary_df) - 1, 'pending'] += len(pending_to_pending_df)\n",
    "        if (len(pending_to_pending_df)):\n",
    "            pending_to_pending_df.alert_status = 'PENDING'\n",
    "            self.new_log_df = pd.concat([self.new_log_df, pending_to_pending_df.drop(columns=['alert_is_present_on'])])    \n",
    "\n",
    "        # PENDING to CLOSED\n",
    "        pending_to_closed_df = not_closed_df.query('alert_status==\"NEW\" & alert_is_present_on==\"left_only\"')\n",
    "        self.summary_df.loc[len(self.summary_df) - 1, 'pending_to_closed'] = len(pending_to_closed_df)\n",
    "        self.summary_df.loc[len(self.summary_df) - 1, 'closed'] += len(pending_to_closed_df)\n",
    "        if (len(pending_to_closed_df)):\n",
    "            pending_to_closed_df.alert_status = 'CLOSED'\n",
    "            pending_to_closed_df.alert_closed_date = dt.now().isoformat()\n",
    "            self.new_log_df = pd.concat([self.new_log_df, pending_to_closed_df.drop(columns=['alert_is_present_on'])])\n",
    "\n",
    "        self.computeProgressionRate()         \n",
    "\n",
    "    def saveLogFile(self):\n",
    "        if (len(self.log_df)):\n",
    "            self.log_df.to_csv(self.logFileName, index=None)\n",
    "    \n",
    "    def saveSummaryFile(self):\n",
    "        if (len(self.summary_df)):\n",
    "            self.summary_df.to_csv(self.summaryFileName, index=None)\n",
    "\n",
    "    def computeProgressionRate(self):\n",
    "        l = len(self.summary_df)\n",
    "        if (len(self.summary_df) < 2): # no previous summary\n",
    "            self.summary_df.loc[l - 1, 'progression_rate'] = 0\n",
    "        else:\n",
    "            if (self.summary_df.loc[l - 2, 'pending'] + self.summary_df.loc[l - 2, 'new']):\n",
    "                # there were pending and/or new alerts last time\n",
    "                self.summary_df.loc[l - 1, 'progression_rate'] = (self.summary_df.loc[l - 1, 'new_to_closed'] + self.summary_df.loc[l - 1, 'pending_to_closed']) / (self.summary_df.loc[l - 2, 'pending'] + self.summary_df.loc[l - 2, 'new'])\n",
    "            else: \n",
    "                self.summary_df.loc[l - 1, 'progression_rate'] = 0\n",
    "            \n",
    "    def updateAlertsAndSave(self):\n",
    "        self.updateAlerts()\n",
    "        self.saveLogFile()\n",
    "        self.saveSummaryFile()\n",
    "\n",
    "    def setLogFilename(self, filename):\n",
    "        self.logFileName = filename\n",
    "\n",
    "    def setSummaryFilename(self, filename):\n",
    "        self.summaryFileName = filename\n",
    "\n",
    "    def getLogs(self):\n",
    "        return self.log_df\n",
    "    \n",
    "    def getSummary(self):\n",
    "        return self.summary_df\n",
    "    \n",
    "    def setMatchingColumnIndexes(self, cols):\n",
    "        self.matchingColumnIndexes = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = './pharoses1_config.json'\n",
    "try:\n",
    "    with open(config_file, 'r') as file:\n",
    "        connectorConfig = json.load(file)\n",
    "        print('Loading config file')\n",
    "except:\n",
    "    connectorConfig = {\n",
    "        \"url\":\"https://ProcessMining.com\",\n",
    "        \"user_id\": \"john.smith\",\n",
    "        \"api_key\":\"8a5kga87eqvd1180\",\n",
    "        \"project_key\": \"procure-to-pay\",\n",
    "        \"org_key\": \"\"\n",
    "    }\n",
    "# Constructors\n",
    "connector = processMiningConnector(connectorConfig)\n",
    "alerts1 = widgetAlerts(connector, 'alerts', 'invoices-withholding-tax', [0,1,2,3])\n",
    "# Optionnaly, the default filenames can be changed like this:\n",
    "alerts1.setLogFilename('mylog.csv')\n",
    "alerts1.setSummaryFilename('mysummary.csv')\n",
    "# The matching columns can be changed. Use columns with values that do not change at each update for the same alert (ex: no duration until now)\n",
    "alerts1.setMatchingColumnIndexes([0,1,2])\n",
    "alerts1.updateAlertsAndSave() # Load files, get widget values, match with log, update log and summary, save files\n",
    "print(alerts1.getLogs().head())\n",
    "print(alerts1.getSummary().head())\n",
    "\n",
    "alerts2 = widgetAlerts(connector, 'alerts2', 'invoices-blocked-account2', [0,1,2,3])\n",
    "alerts2.updateAlertsAndSave()\n",
    "print(alerts2.getLogs().head())\n",
    "print(alerts2.getSummary().head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
