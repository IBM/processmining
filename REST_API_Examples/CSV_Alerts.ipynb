{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import time\n",
    "import hashlib, hmac, base64\n",
    "import requests, json, urllib3\n",
    "\n",
    "requests.packages.urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "import sys\n",
    "\n",
    "SPINNING_RATE = 0.1\n",
    "TIME_ADJUST = 0\n",
    "PRINT_TRACE = 0\n",
    "\n",
    "# config is passed to most function, this is a json object that includes all the required parameters\n",
    "\n",
    "def spinning_cursor():\n",
    "    while True:\n",
    "        for cursor in '|/-\\\\':\n",
    "            yield cursor\n",
    "\n",
    "def init_params_headers(config, headers, params):\n",
    "    params.update({'org' : config['org_key']})     \n",
    "    headers.update({\"content-type\": \"application/json\", \"Authorization\": \"Bearer %s\" % config['sign'] })\n",
    "\n",
    "def ws_post_sign(config):\n",
    "\n",
    "    url = \"%s/integration/sign\" % config['url']\n",
    "    headers = {\"content-type\": \"application/json\"}\n",
    "    data = { 'uid' : config['user_id'], 'apiKey' : config['api_key']}\n",
    "    if( PRINT_TRACE) : print(\"REST CALL: \"+url+\" \"+str(data))\n",
    "    r = requests.post(url, verify=False, data=json.dumps(data), headers=headers)\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Error: get signature: %s\" % r.json()['data'])\n",
    "        config['sign'] = 0\n",
    "        return 0\n",
    "    else:\n",
    "        config['sign'] = r.json()['sign']\n",
    "    return r.json()['sign']\n",
    "\n",
    "\n",
    "def ws_delete_process(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/processes/%s\" % (config['url'], config['project_key'])\n",
    "\n",
    "    if( PRINT_TRACE) : print(\"REST CALL: \"+ url+\" \"+str(params))\n",
    "    r = requests.delete(url, verify=False, params=params, headers=headers)\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Error: delete process: %s\" % r.json()['data'])        \n",
    "    return r\n",
    "\n",
    "def ws_proc_post(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/processes\" % (config['url'])\n",
    "    data = { 'title' : config['project_name'], 'org' : config['org_key']}\n",
    "    if( PRINT_TRACE) : print(\"REST CALL: \"+ url+\" \"+str(data))\n",
    "    r = requests.post(url, verify=False, data=json.dumps(data), headers=headers, params=params)\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Error: Process %s creation: %s\" % (config['project_name'], r.json()['data']))        \n",
    "    return r\n",
    "\n",
    "def ws_csv_upload(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "    # no content-type \n",
    "    headers.pop(\"content-type\")\n",
    "\n",
    "    url = \"%s/integration/csv/%s/upload\" % (config['url'], config['project_key'])\n",
    "    if( PRINT_TRACE) : print(\"REST CALL: \"+ url+\" \"+str(params))\n",
    "    files = {'file': (config['csv_filename'], open(config['csv_filename'], 'rb'),'text/zip')}\n",
    "    r = requests.post(url, verify=False, files=files, params=params, headers=headers)\n",
    "    # Async call --- the caller need to loop on get job status that it is completed\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Error: CSV upload: %s\" % r.json()['data'])        \n",
    "    return r\n",
    "\n",
    "\n",
    "def ws_query_post(config, query):\n",
    "    headers = {}\n",
    "    params = {}\n",
    "    init_params_headers(config, headers, params)\n",
    "    headers['content-type'] = 'application/x-www-form-urlencoded'\n",
    "    url = \"%s/analytics/integration/%s/query\" % (\n",
    "        config['url'], config['project_key'])\n",
    "    data = \"params={'query': '%s'}\" % query\n",
    "    print(data)\n",
    "    r = requests.post(url, verify=False, params=params,\n",
    "                      headers=headers, data=data)\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Error: query post: %s\" % r.json()['data'])\n",
    "    return r\n",
    "\n",
    "\n",
    "def ws_backup_upload(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "    # no content-type \n",
    "    headers.pop(\"content-type\")\n",
    "\n",
    "    url = \"%s/integration/processes/%s/upload-backup\" % (config['url'], config['project_key'])\n",
    "\n",
    "    if( PRINT_TRACE) : print(\"REST CALL: \"+ url+\" \"+str(params))\n",
    "    files = {'file': (config['backup_filename'], open(config['backup_filename'], 'rb'),'text/zip')}\n",
    "    r = requests.post(url, verify=False, files=files, params=params, headers=headers)\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Error: backup upload: %s\" % r.json()['data']) \n",
    "    return r\n",
    "\n",
    "def ws_get_backup_list(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url  = \"%s/integration/processes/%s/backups\" % (config['url'], config['project_key'])\n",
    "    if( PRINT_TRACE) : print(\"REST CALL: \"+ url+\" \"+str(params))\n",
    "    r = requests.get(url, verify=False, headers=headers, params=params)\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Error: backup list get: %s\" % r.json()['data'])\n",
    "    return r\n",
    "\n",
    "def getBackupIdByMessage(backuplist, message):\n",
    "    backuplist = backuplist['backups']\n",
    "    for backup in backuplist:\n",
    "        if (backup['message'] == message) :\n",
    "            return backup['id']\n",
    "    return 0\n",
    "\n",
    "def ws_apply_backup(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/processes/%s/backups/%s\" % (config['url'], config['project_key'], config['backup_id'])\n",
    "    if( PRINT_TRACE) : print(\"REST CALL: \"+ url+\" \"+str(params))\n",
    "    r = requests.put(url, verify=False, headers=headers, params=params)\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Error: apply backup: %s : %s\" % (r.json()['data'], config['backup_id'])) \n",
    "    return r\n",
    "\n",
    "def ws_create_log(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/csv/%s/create-log\" % (config['url'], config['project_key'])\n",
    "    if( PRINT_TRACE) : print(\"REST CALL: \"+ url+\" \"+str(params))\n",
    "    r = requests.post(url, verify=False, headers=headers, params=params)\n",
    "    # Async function, the caller needs to check the job status (returned in r)\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Error: create log: %s\" % r.json()['data']) \n",
    "    return r\n",
    "\n",
    "def ws_get_csv_job_status(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/csv/job-status/%s\" % (config['url'], config['job_key'])\n",
    "    if( PRINT_TRACE) : print(\"REST CALL: \"+ url+\" \"+str(params))\n",
    "    r = requests.get(url, verify=False, headers=headers, params=params)\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Error: get CSV job status: %s\" % r.json()['data'])        \n",
    "    return r\n",
    "\n",
    "def create_and_load_new_project(config):\n",
    "    print(\"Process Mining: creating new project\")\n",
    "    r_proc = ws_proc_post(config)\n",
    "    if (r_proc.status_code != 200): return r_proc\n",
    "    config['project_key']= r_proc.json()['projectKey']\n",
    "    print(\"Process Mining: loading event log (please wait)\")\n",
    "    r = ws_csv_upload(config)\n",
    "    if (r.status_code != 200): return r\n",
    "    else :\n",
    "        # wait until async call is completed\n",
    "        config['job_key']= r.json()['data']\n",
    "        runningCall = 1\n",
    "        spinner = spinning_cursor()\n",
    "        while  runningCall :\n",
    "            r = ws_get_csv_job_status(config)\n",
    "            if (r.json()['data'] == 'complete') :\n",
    "                runningCall = 0\n",
    "            if (r.json()['data'] == 'error') :\n",
    "                runningCall = 0\n",
    "                print(\"Error while loading CSV -- column number mismatch\")\n",
    "                return 0\n",
    "            sys.stdout.write(next(spinner))\n",
    "            sys.stdout.flush()\n",
    "            sys.stdout.write('\\b')\n",
    "            time.sleep(SPINNING_RATE)\n",
    "\n",
    "    r = ws_backup_upload(config)\n",
    "    if (r.status_code != 200): return r\n",
    "    config['backup_id'] = r.json()['backupInfo']['id']\n",
    "    r = ws_apply_backup(config)\n",
    "    if (r.status_code != 200): return r\n",
    "    print(\"Process Mining: refreshing process (please wait)\")\n",
    "    r = ws_create_log(config)\n",
    "    if (r.status_code != 200): return r\n",
    "    else :\n",
    "        # wait until async call is completed\n",
    "        config['job_key']= r.json()['data']\n",
    "        runningCall = 1\n",
    "        #pbar = tqdm(total=100)\n",
    "        spinner = spinning_cursor()\n",
    "        while  runningCall :\n",
    "            r = ws_get_csv_job_status(config)\n",
    "            if (r.json()['data'] == 'complete') :\n",
    "                runningCall = 0\n",
    "            if (r.json()['data'] == 'error') :\n",
    "                runningCall = 0\n",
    "                print(\"Error while creating the log\")\n",
    "                return 0\n",
    "            sys.stdout.write(next(spinner))\n",
    "            sys.stdout.flush()\n",
    "            sys.stdout.write('\\b')\n",
    "            time.sleep(SPINNING_RATE)\n",
    "    return r_proc\n",
    "\n",
    "def upload_csv_and_createlog(config) :\n",
    "    r = ws_csv_upload(config)\n",
    "    if (r.status_code != 200): return r\n",
    "    else :\n",
    "        # wait until async call is completed\n",
    "        config['job_key']= r.json()['data']\n",
    "        runningCall = 1\n",
    "        print(\"Process Mining: loading event log (please wait)\")\n",
    "        spinner = spinning_cursor()\n",
    "        while  runningCall :\n",
    "            r = ws_get_csv_job_status(config)\n",
    "            if (r.json()['data'] == 'complete') :\n",
    "                runningCall = 0\n",
    "            if (r.json()['data'] == 'error') :\n",
    "                runningCall = 0\n",
    "                print(\"Error while loading CSV -- column number mismatch\")\n",
    "                return 0\n",
    "            sys.stdout.write(next(spinner))\n",
    "            sys.stdout.flush()\n",
    "            sys.stdout.write('\\b')\n",
    "            time.sleep(SPINNING_RATE)\n",
    "\n",
    "    r = ws_create_log(config)\n",
    "    if (r.status_code != 200): return r\n",
    "    else :\n",
    "        # wait until async call is completed\n",
    "        config['job_key']= r.json()['data']\n",
    "        runningCall = 1\n",
    "        print(\"Process Mining: refreshing model (please wait)\")\n",
    "        spinner = spinning_cursor()\n",
    "        while  runningCall :\n",
    "            r = ws_get_csv_job_status(config)\n",
    "            if (r.json()['data'] == 'complete') :\n",
    "                runningCall = 0\n",
    "            if (r.json()['data'] == 'error') :\n",
    "                runningCall = 0\n",
    "                print(\"Error while creating the log\")\n",
    "                return 0\n",
    "            sys.stdout.write(next(spinner))\n",
    "            sys.stdout.flush()\n",
    "            sys.stdout.write('\\b')\n",
    "            time.sleep(SPINNING_RATE)\n",
    "    return r\n",
    "\n",
    "# List all the dashboards of a given process mining project\n",
    "def ws_get_dashboards(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/analytics/integration/dashboard/%s/list\" % (config['url'], config['project_key'])\n",
    "    r = requests.get(url, verify=False, params=params, headers=headers)\n",
    "    if (r.status_code == 200):\n",
    "        values = r.json()\n",
    "        return values['data']['dashboards']\n",
    "    else:\n",
    "        return r\n",
    "\n",
    "# Returns only the table widgets in the dashboard_name\n",
    "def ws_get_widgets(config):\n",
    "    dashboards = ws_get_dashboards(config)\n",
    "    dashboard = 0\n",
    "    for aDashboard in dashboards:\n",
    "        if (aDashboard['name'] == config['dashboard_name']):\n",
    "            dashboard = aDashboard\n",
    "\n",
    "    if dashboard == 0 :\n",
    "        print(\"ERROR: dashboard does not exist\")\n",
    "        return 0\n",
    "    dashboard_id = dashboard['id']\n",
    "\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/analytics/integration/dashboard/%s/%s/list\" % (config['url'], config['project_key'], dashboard_id)\n",
    "\n",
    "    r = requests.get(url, verify=False, params=params, headers=headers )\n",
    "    widgets = r.json()\n",
    "    return widgets['data']['widgets']\n",
    "\n",
    "def ws_get_widget_values(config):\n",
    "    dashboard_id = config.get('dashboard_id')\n",
    "    if dashboard_id is None:\n",
    "        # add the dashboard_id to the widget data\n",
    "        print(\"....searching dashboard_id\")\n",
    "        dashboards = ws_get_dashboards(config)\n",
    "        dashboard = 0\n",
    "        for aDashboard in dashboards:\n",
    "            if (aDashboard['name'] == config['dashboard_name']):\n",
    "                dashboard = aDashboard\n",
    "        if (dashboard == 0) :\n",
    "            print(\"ERROR: dashboard does not exist\")\n",
    "            return 0\n",
    "        else:\n",
    "            # Store the dashboard id in the widget to avoid calling again the rest API to retrieve the dashboard by name\n",
    "            config['dashboard_id'] = dashboard['id']\n",
    "\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/analytics/integration/dashboard/%s/%s/%s/retrieve\" % (config['url'],\n",
    "                                                                        config['project_key'],\n",
    "                                                                        config['dashboard_id'],\n",
    "                                                                        config['widget_id'])\n",
    "\n",
    "    r = requests.get(url, verify=False, params=params, headers=headers )\n",
    "    values = r.json()\n",
    "    return values['data']\n",
    "\n",
    "def ws_create_update_variables(config, variablesArray):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/processes/%s/variables\" % (config['url'], config['project_key'])\n",
    "    r = requests.post(url, verify=False, data=json.dumps(variablesArray), params=params, headers=headers )\n",
    "    return r\n",
    "\n",
    "def ws_get_variable(config, variablename):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "    \n",
    "    url = \"%s/integration/processes/%s/variables/%s\" % (config['url'], config['project_key'], variablename)\n",
    "    r = requests.get(url, verify=False, params=params, headers=headers )\n",
    "    if (r.status_code == 200):\n",
    "        values = r.json()\n",
    "        return values['data']\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def ws_get_variables(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/processes/%s/variables\" % (config['url'], config['project_key'])\n",
    "    r = requests.get(url, verify=False, params=params, headers=headers )\n",
    "    if (r.status_code == 200):\n",
    "        values = r.json()\n",
    "        return values['data']\n",
    "    else:\n",
    "        print(\"Error: ws_get_variables %s\" % r.json()['data'])\n",
    "        return []\n",
    "\n",
    "def ws_delete_variable(config, variablename):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/processes/%s/variables/%s\" % (config['url'], config['project_key'], variablename)\n",
    "    r = requests.delete(url, verify=False, params=params, headers=headers )\n",
    "    return r.status_code\n",
    "\n",
    "def ws_delete_variables(config):\n",
    "    variables = ws_get_variables(config)\n",
    "    for variable in variables:\n",
    "        ws_delete_variable(config, variable['name'])\n",
    "\n",
    "def ws_get_processes(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/processes\" % (config['url'])\n",
    "    r = requests.get(url, verify=False, params=params, headers=headers )\n",
    "    if (r.status_code == 200):\n",
    "        values = r.json()\n",
    "        return values['data']\n",
    "    else:\n",
    "        return r\n",
    "\n",
    "def ws_get_project_info(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/processes/%s\" % (config['url'], config['project_key'])\n",
    "\n",
    "    return(requests.get(url, verify=False, params=params, headers=headers ))\n",
    "\n",
    "\n",
    "def ws_get_project_meta_info(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/csv/%s/meta\" % (config['url'], config['project_key'])\n",
    "\n",
    "    r = requests.get(url, verify=False, params=params, headers=headers )\n",
    "    if (r.status_code == 200):\n",
    "        values = r.json()\n",
    "        return values['data']\n",
    "    else:\n",
    "        return r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime as dt, timedelta\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "def load_history_df(history_file):\n",
    "    try: \n",
    "        histo_df = pd.read_csv(history_file, dtype=str)\n",
    "        print('Loading history file: %s' % history_file)\n",
    "        return histo_df\n",
    "    except:\n",
    "        # First call: Create the historical CSV from scratch at the end\n",
    "        print('First time execution, no history df')\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "def join_and_cleanup(df1, df2, cols, suffix, suffix_for):\n",
    "    # suffix_for = 'left' or 'right'\n",
    "    if (suffix_for == 'left'): suffixes = (suffix, '')\n",
    "    else: suffixes = ('', suffix)\n",
    "    df = df1.merge(df2, how='left', on=cols, indicator='alert_is_present_on', suffixes=suffixes)\n",
    "    # remove columns (duplicated) from history\n",
    "    kept_cols = []\n",
    "    for col in df.columns:\n",
    "        if (suffix not in col):\n",
    "            kept_cols.append(col)\n",
    "    return df[kept_cols]\n",
    "\n",
    "def manageAlertsFromWidget(myconfig):\n",
    "        \n",
    "    ws_post_sign(myconfig)\n",
    "\n",
    "    # Get the current values from the widget\n",
    "    widgetAlerts = ws_get_widget_values(myconfig)\n",
    "    widget_df = pd.DataFrame(widgetAlerts)\n",
    "\n",
    "    print(\"%d current alerts for widget %s in dashboard %s\" % (len(widgetAlerts), myconfig['widget_id'], myconfig['dashboard_name']))     \n",
    "    widget_df = pd.DataFrame(widgetAlerts)\n",
    "\n",
    "    # if the alert_history file does not exist, create it\n",
    "    history_file = 'alert_history_' + myconfig['dashboard_name'] + '_' + myconfig['widget_id']+'.csv'\n",
    "    summary_file = 'alert_summary_' + myconfig['dashboard_name'] + '_' + myconfig['widget_id']+'.csv'\n",
    "\n",
    "    # Load alert history for this widget\n",
    "    histo_df = load_history_df(history_file)\n",
    "    if (len(histo_df) == 0):\n",
    "        # First time execution\n",
    "        if (len(widget_df) ==  0): # No alerts in the widget\n",
    "            print('Empty widget, nothing to do')\n",
    "            return\n",
    "        else: # create the first history file and summary file and quit\n",
    "            histo_df = pd.DataFrame(widgetAlerts)\n",
    "            histo_df['alert_status'] = 'NEW'\n",
    "            histo_df['alert_creation_date'] = dt.now().isoformat()\n",
    "            histo_df['alert_closed_date'] = ''\n",
    "            histo_df.to_csv(history_file, index=None)\n",
    "            summary = {\n",
    "                'update_date': dt.now().isoformat(),\n",
    "                #'last_update_date': 0,\n",
    "                #'last_new': 0,\n",
    "                #'last_pending': 0,\n",
    "                #'last_closed': 0,\n",
    "                'new': len(histo_df),\n",
    "                'pending': 0,\n",
    "                'closed': 0,\n",
    "                'new_to_pending': 0,\n",
    "                'new_to_closed': 0,\n",
    "                'pending_to_closed': 0,\n",
    "                'pending_to_pending': 0,\n",
    "                'any_to_closed': 0,\n",
    "                'progression_rate': 0\n",
    "            }\n",
    "            summary_df = pd.DataFrame([summary])\n",
    "            summary_df.to_csv(summary_file, index=None)\n",
    "            return\n",
    "\n",
    "    # Load summary history for this widget\n",
    "    try: \n",
    "        summary_df = pd.read_csv(summary_file)\n",
    "        print('Loading summary file: %s' % summary_file)\n",
    "        last_summary = summary_df.loc[len(summary_df) - 1]\n",
    "    except:\n",
    "        # First call: Create the summary df \n",
    "        summary_df = pd.DataFrame()\n",
    "        last_summary = {\n",
    "            'update_date': 0,\n",
    "            'new': 0,\n",
    "            'pending': 0,\n",
    "            'closed': 0,\n",
    "            'new_to_pending': 0,\n",
    "            'new_to_closed': 0,\n",
    "            'pending_to_closed': 0,\n",
    "            'pending_to_pending': 0,\n",
    "            'any_to_closed': 0,\n",
    "            'progression_rate': 0\n",
    "        }\n",
    "    summary = {\n",
    "        'update_date': dt.now().isoformat(),\n",
    "        'new': 0,\n",
    "        'pending': 0,\n",
    "        'closed': last_summary['closed'],\n",
    "        'new_to_pending': 0,\n",
    "        'new_to_closed': 0,\n",
    "        'pending_to_closed': 0,\n",
    "        'pending_to_pending': 0,\n",
    "        'any_to_closed': 0,\n",
    "        'progression_rate': 0\n",
    "    }\n",
    "\n",
    "    # Collect the number of alerts in history for each status\n",
    "    histo_closed_df = histo_df[histo_df['alert_status'] ==  'CLOSED']\n",
    "    final_df = histo_closed_df # we keep the closed alerts\n",
    "    if (len(histo_closed_df)):\n",
    "        print('%d alerts already CLOSED' % len(histo_closed_df))\n",
    "\n",
    "    # If widget is empty, all the alerts are CLOSED\n",
    "    if (len(widget_df) == 0):\n",
    "        print(\"Empty widget, all alerts are closed\")\n",
    "        summary['new'] = 0\n",
    "        summary['pending'] = 0\n",
    "        summary['closed'] = len(histo_df)\n",
    "        summary['new_to_closed'] = last_summary['new']\n",
    "        summary['pending_to_closed'] = last_summary['pending']\n",
    "        summary['any_to_closed'] = summary['new_to_closed'] + summary['pending_to_closed']\n",
    "        summary['pending_to_pending'] = 0\n",
    "\n",
    "        histo_any_to_close_df = histo_df[histo_df['alert_status'] != 'CLOSED']\n",
    "        histo_any_to_close_df['alert_closed_date'] = dt.now().isoformat()\n",
    "        histo_any_to_close_df['alert_status'] =  'CLOSED'\n",
    "        final_df = pd.concat([final_df, histo_any_to_close_df])\n",
    "        final_df.to_csv(history_file, index=None)\n",
    "\n",
    "    else: # Widget is not empty\n",
    "        \n",
    "        # We remove CLOSED alerts from histo_df, such that we can add same alerts again if they appear in the widget\n",
    "        histo_not_closed_df = histo_df[histo_df['alert_status'] != 'CLOSED']\n",
    "\n",
    "        # NEW alerts in the widget\n",
    "        # Do the merge with the columns mentionned in the configuration as an array\n",
    "        if (myconfig['alert_matching_columns'] == 0):\n",
    "            matching_cols=[widget_df.columns[0]]\n",
    "        elif (myconfig['alert_matching_columns'] == ''):\n",
    "            matching_cols=[widget_df.columns[0]]\n",
    "        else:\n",
    "            matching_cols = []\n",
    "            for i in range(len(widget_df.columns)):\n",
    "                if (i in myconfig['alert_matching_columns']):\n",
    "                    matching_cols.append(widget_df.columns[i])\n",
    "        print(\"Key columns used for matching existing alerts: %s\" % matching_cols)\n",
    "\n",
    "        widget_new_df = join_and_cleanup(widget_df, histo_not_closed_df, matching_cols, \"_suffixFromHistory\", 'right')\n",
    "        # Alerts in the widget with 'exist'==left_only are NEW\n",
    "        widget_new_df = widget_new_df[widget_new_df['alert_is_present_on'] == 'left_only']\n",
    "        summary['new'] = len(widget_new_df)\n",
    "        if (summary['new']):\n",
    "            widget_new_df['alert_status'] = 'NEW'\n",
    "            widget_new_df['alert_creation_date'] = dt.now().isoformat()\n",
    "            widget_new_df['alert_closed_date'] = ''\n",
    "            print('%d new alerts' % summary['new'])\n",
    "            final_df = pd.concat([final_df, widget_new_df.drop(columns=['alert_is_present_on'])])\n",
    "\n",
    "\n",
    "        # Process alerts that are in the HISTORY\n",
    "        #histo_not_closed_df = histo_not_closed_df.merge(widget_df, on=widget_df.columns.tolist(), how='left', indicator='alert_is_present_on')\n",
    "        histo_not_closed_df = join_and_cleanup(histo_not_closed_df, widget_df, matching_cols, \"_suffixFromHistory\", 'left')\n",
    "\n",
    "        # NEW to PENDING  \n",
    "        histo_new_to_pending_df = histo_not_closed_df.query('alert_status==\"NEW\" & alert_is_present_on==\"both\"')\n",
    "        summary['new_to_pending'] = len(histo_new_to_pending_df)\n",
    "        summary['pending'] =  summary['new_to_pending']                                \n",
    "        if (summary['new_to_pending']):\n",
    "            print('%d alerts moved from NEW to PENDING' % summary['new_to_pending'])\n",
    "            histo_new_to_pending_df['alert_status'] = 'PENDING'\n",
    "            final_df = pd.concat([final_df, histo_new_to_pending_df.drop(columns=['alert_is_present_on'])])\n",
    "\n",
    "        # NEW to CLOSE\n",
    "        histo_new_to_close_df = histo_not_closed_df.query('alert_status==\"NEW\" & alert_is_present_on==\"left_only\"')\n",
    "        summary['new_to_closed'] = len(histo_new_to_close_df)\n",
    "        summary['closed'] += summary['new_to_closed']                              \n",
    "        if (summary['new_to_closed']):\n",
    "            print('%d alerts moved from NEW to CLOSED' % summary['new_to_closed'])\n",
    "            histo_new_to_close_df['alert_status'] = 'CLOSED'\n",
    "            histo_new_to_close_df['alert_closed_date'] = dt.now().isoformat()\n",
    "            final_df = pd.concat([final_df, histo_new_to_close_df.drop(columns=['alert_is_present_on'])]) \n",
    "\n",
    "        # PENDING to PENDING\n",
    "        histo_pending_to_pending_df = histo_not_closed_df.query('alert_status==\"PENDING\" & alert_is_present_on==\"both\"')\n",
    "        summary['pending_to_pending'] = len(histo_pending_to_pending_df)\n",
    "        summary['pending'] += summary['pending_to_pending']\n",
    "        if (summary['pending_to_pending']):\n",
    "            print('%d alerts still PENDING' % summary['pending_to_pending'])\n",
    "            final_df = pd.concat([final_df, histo_pending_to_pending_df.drop(columns=['alert_is_present_on'])])\n",
    "\n",
    "        # PENDING to CLOSE\n",
    "        histo_pending_to_close_df = histo_not_closed_df.query('alert_status==\"PENDING\" & alert_is_present_on==\"left_only\"')\n",
    "        summary['pending_to_closed'] = len(histo_pending_to_close_df)\n",
    "        summary['closed'] += summary['pending_to_closed']                              \n",
    "\n",
    "        if (summary['pending_to_closed']):\n",
    "            print('%d alerts moved from PENDING to CLOSE' % summary['pending_to_closed'])\n",
    "            final_df = pd.concat([final_df, histo_pending_to_close_df.drop(columns=['alert_is_present_on'])]) \n",
    "\n",
    "    # Save final_df in the history file\n",
    "    final_df.to_csv(history_file, index=None)\n",
    "\n",
    "    # Add the summary object to the summary dataframe\n",
    "    if (last_summary['pending'] + last_summary['new']):\n",
    "        summary['progression_rate'] = summary['any_to_closed'] / (last_summary['pending'] + last_summary['new'])\n",
    "    else:\n",
    "        summary['progression_rate'] = 0.0\n",
    "    print('Progression rate: %d' % summary['progression_rate'])\n",
    "    summary_df.loc[len(summary_df)] = summary\n",
    "    summary_df.to_csv(summary_file, index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config file\n",
      "....searching dashboard_id\n",
      "3 current alerts for widget alerts-widget-1 in dashboard Alerts Dashboard\n",
      "Loading history file: alert_history_Alerts Dashboard_alerts-widget-1.csv\n",
      "Loading summary file: alert_summary_Alerts Dashboard_alerts-widget-1.csv\n",
      "3 alerts already CLOSED\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'key_columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 18\u001b[0m\n\u001b[1;32m      7\u001b[0m     myconfig \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://ProcessMining.com\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjohn.smith\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert_matching_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m], \u001b[38;5;66;03m# Index of columns that are used to match current/new alerts\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     }\n\u001b[1;32m     17\u001b[0m myconfig\n\u001b[0;32m---> 18\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mmanageAlertsFromWidget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmyconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m res\n",
      "Cell \u001b[0;32mIn[87], line 141\u001b[0m, in \u001b[0;36mmanageAlertsFromWidget\u001b[0;34m(myconfig)\u001b[0m\n\u001b[1;32m    137\u001b[0m histo_not_closed_df \u001b[38;5;241m=\u001b[39m histo_df[histo_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malert_status\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCLOSED\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# NEW alerts in the widget\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Do the merge with the columns mentionned in the configuration as an array\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[43mmyconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkey_columns\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    142\u001b[0m     matching_cols\u001b[38;5;241m=\u001b[39m[widget_df\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'key_columns'"
     ]
    }
   ],
   "source": [
    "config_file = './pharoses1_config.json'\n",
    "try:\n",
    "    with open(config_file, 'r') as file:\n",
    "        myconfig = json.load(file)\n",
    "        print('Loading config file')\n",
    "except:\n",
    "    myconfig = {\n",
    "        \"url\":\"https://ProcessMining.com\",\n",
    "        \"user_id\": \"john.smith\",\n",
    "        \"api_key\":\"8a5kga87eqvd1180\",\n",
    "        \"project_key\": \"procure-to-pay\",\n",
    "        \"org_key\": \"\",\n",
    "        \"dashboard_name\": \"Alerts Dashboard\",\n",
    "        \"widget_id\": \"alerts-widget-1\",\n",
    "        \"alert_matching_columns\": [0,1,2,3], # Index of columns that are used to match current/new alerts\n",
    "    }\n",
    "myconfig\n",
    "res = manageAlertsFromWidget(myconfig)\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
