{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import time\n",
    "import hashlib, hmac, base64\n",
    "import requests, json, urllib3\n",
    "\n",
    "requests.packages.urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "import sys\n",
    "\n",
    "SPINNING_RATE = 0.1\n",
    "TIME_ADJUST = 0\n",
    "PRINT_TRACE = 0\n",
    "\n",
    "# config is passed to most function, this is a json object that includes all the required parameters\n",
    "\n",
    "def spinning_cursor():\n",
    "    while True:\n",
    "        for cursor in '|/-\\\\':\n",
    "            yield cursor\n",
    "\n",
    "def init_params_headers(config, headers, params):\n",
    "    params.update({'org' : config['org_key']})     \n",
    "    headers.update({\"content-type\": \"application/json\", \"Authorization\": \"Bearer %s\" % config['sign'] })\n",
    "\n",
    "def ws_post_sign(config):\n",
    "\n",
    "    url = \"%s/integration/sign\" % config['url']\n",
    "    headers = {\"content-type\": \"application/json\"}\n",
    "    data = { 'uid' : config['user_id'], 'apiKey' : config['api_key']}\n",
    "    if( PRINT_TRACE) : print(\"REST CALL: \"+url+\" \"+str(data))\n",
    "    r = requests.post(url, verify=False, data=json.dumps(data), headers=headers)\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Error: get signature: %s\" % r.json()['data'])\n",
    "        config['sign'] = 0\n",
    "        return 0\n",
    "    else:\n",
    "        config['sign'] = r.json()['sign']\n",
    "    return r.json()['sign']\n",
    "\n",
    "\n",
    "def ws_delete_process(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/processes/%s\" % (config['url'], config['project_key'])\n",
    "\n",
    "    if( PRINT_TRACE) : print(\"REST CALL: \"+ url+\" \"+str(params))\n",
    "    r = requests.delete(url, verify=False, params=params, headers=headers)\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Error: delete process: %s\" % r.json()['data'])        \n",
    "    return r\n",
    "\n",
    "def ws_proc_post(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/processes\" % (config['url'])\n",
    "    data = { 'title' : config['project_name'], 'org' : config['org_key']}\n",
    "    if( PRINT_TRACE) : print(\"REST CALL: \"+ url+\" \"+str(data))\n",
    "    r = requests.post(url, verify=False, data=json.dumps(data), headers=headers, params=params)\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Error: Process %s creation: %s\" % (config['project_name'], r.json()['data']))        \n",
    "    return r\n",
    "\n",
    "def ws_csv_upload(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "    # no content-type \n",
    "    headers.pop(\"content-type\")\n",
    "\n",
    "    url = \"%s/integration/csv/%s/upload\" % (config['url'], config['project_key'])\n",
    "    if( PRINT_TRACE) : print(\"REST CALL: \"+ url+\" \"+str(params))\n",
    "    files = {'file': (config['csv_filename'], open(config['csv_filename'], 'rb'),'text/zip')}\n",
    "    r = requests.post(url, verify=False, files=files, params=params, headers=headers)\n",
    "    # Async call --- the caller need to loop on get job status that it is completed\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Error: CSV upload: %s\" % r.json()['data'])        \n",
    "    return r\n",
    "\n",
    "\n",
    "def ws_query_post(config, query):\n",
    "    headers = {}\n",
    "    params = {}\n",
    "    init_params_headers(config, headers, params)\n",
    "    headers['content-type'] = 'application/x-www-form-urlencoded'\n",
    "    url = \"%s/analytics/integration/%s/query\" % (\n",
    "        config['url'], config['project_key'])\n",
    "    data = \"params={'query': '%s'}\" % query\n",
    "    print(data)\n",
    "    r = requests.post(url, verify=False, params=params,\n",
    "                      headers=headers, data=data)\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Error: query post: %s\" % r.json()['data'])\n",
    "    return r\n",
    "\n",
    "\n",
    "def ws_backup_upload(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "    # no content-type \n",
    "    headers.pop(\"content-type\")\n",
    "\n",
    "    url = \"%s/integration/processes/%s/upload-backup\" % (config['url'], config['project_key'])\n",
    "\n",
    "    if( PRINT_TRACE) : print(\"REST CALL: \"+ url+\" \"+str(params))\n",
    "    files = {'file': (config['backup_filename'], open(config['backup_filename'], 'rb'),'text/zip')}\n",
    "    r = requests.post(url, verify=False, files=files, params=params, headers=headers)\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Error: backup upload: %s\" % r.json()['data']) \n",
    "    return r\n",
    "\n",
    "def ws_get_backup_list(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url  = \"%s/integration/processes/%s/backups\" % (config['url'], config['project_key'])\n",
    "    if( PRINT_TRACE) : print(\"REST CALL: \"+ url+\" \"+str(params))\n",
    "    r = requests.get(url, verify=False, headers=headers, params=params)\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Error: backup list get: %s\" % r.json()['data'])\n",
    "    return r\n",
    "\n",
    "def getBackupIdByMessage(backuplist, message):\n",
    "    backuplist = backuplist['backups']\n",
    "    for backup in backuplist:\n",
    "        if (backup['message'] == message) :\n",
    "            return backup['id']\n",
    "    return 0\n",
    "\n",
    "def ws_apply_backup(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/processes/%s/backups/%s\" % (config['url'], config['project_key'], config['backup_id'])\n",
    "    if( PRINT_TRACE) : print(\"REST CALL: \"+ url+\" \"+str(params))\n",
    "    r = requests.put(url, verify=False, headers=headers, params=params)\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Error: apply backup: %s : %s\" % (r.json()['data'], config['backup_id'])) \n",
    "    return r\n",
    "\n",
    "def ws_create_log(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/csv/%s/create-log\" % (config['url'], config['project_key'])\n",
    "    if( PRINT_TRACE) : print(\"REST CALL: \"+ url+\" \"+str(params))\n",
    "    r = requests.post(url, verify=False, headers=headers, params=params)\n",
    "    # Async function, the caller needs to check the job status (returned in r)\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Error: create log: %s\" % r.json()['data']) \n",
    "    return r\n",
    "\n",
    "def ws_get_csv_job_status(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/csv/job-status/%s\" % (config['url'], config['job_key'])\n",
    "    if( PRINT_TRACE) : print(\"REST CALL: \"+ url+\" \"+str(params))\n",
    "    r = requests.get(url, verify=False, headers=headers, params=params)\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Error: get CSV job status: %s\" % r.json()['data'])        \n",
    "    return r\n",
    "\n",
    "def create_and_load_new_project(config):\n",
    "    print(\"Process Mining: creating new project\")\n",
    "    r_proc = ws_proc_post(config)\n",
    "    if (r_proc.status_code != 200): return r_proc\n",
    "    config['project_key']= r_proc.json()['projectKey']\n",
    "    print(\"Process Mining: loading event log (please wait)\")\n",
    "    r = ws_csv_upload(config)\n",
    "    if (r.status_code != 200): return r\n",
    "    else :\n",
    "        # wait until async call is completed\n",
    "        config['job_key']= r.json()['data']\n",
    "        runningCall = 1\n",
    "        spinner = spinning_cursor()\n",
    "        while  runningCall :\n",
    "            r = ws_get_csv_job_status(config)\n",
    "            if (r.json()['data'] == 'complete') :\n",
    "                runningCall = 0\n",
    "            if (r.json()['data'] == 'error') :\n",
    "                runningCall = 0\n",
    "                print(\"Error while loading CSV -- column number mismatch\")\n",
    "                return 0\n",
    "            sys.stdout.write(next(spinner))\n",
    "            sys.stdout.flush()\n",
    "            sys.stdout.write('\\b')\n",
    "            time.sleep(SPINNING_RATE)\n",
    "\n",
    "    r = ws_backup_upload(config)\n",
    "    if (r.status_code != 200): return r\n",
    "    config['backup_id'] = r.json()['backupInfo']['id']\n",
    "    r = ws_apply_backup(config)\n",
    "    if (r.status_code != 200): return r\n",
    "    print(\"Process Mining: refreshing process (please wait)\")\n",
    "    r = ws_create_log(config)\n",
    "    if (r.status_code != 200): return r\n",
    "    else :\n",
    "        # wait until async call is completed\n",
    "        config['job_key']= r.json()['data']\n",
    "        runningCall = 1\n",
    "        #pbar = tqdm(total=100)\n",
    "        spinner = spinning_cursor()\n",
    "        while  runningCall :\n",
    "            r = ws_get_csv_job_status(config)\n",
    "            if (r.json()['data'] == 'complete') :\n",
    "                runningCall = 0\n",
    "            if (r.json()['data'] == 'error') :\n",
    "                runningCall = 0\n",
    "                print(\"Error while creating the log\")\n",
    "                return 0\n",
    "            sys.stdout.write(next(spinner))\n",
    "            sys.stdout.flush()\n",
    "            sys.stdout.write('\\b')\n",
    "            time.sleep(SPINNING_RATE)\n",
    "    return r_proc\n",
    "\n",
    "def upload_csv_and_createlog(config) :\n",
    "    r = ws_csv_upload(config)\n",
    "    if (r.status_code != 200): return r\n",
    "    else :\n",
    "        # wait until async call is completed\n",
    "        config['job_key']= r.json()['data']\n",
    "        runningCall = 1\n",
    "        print(\"Process Mining: loading event log (please wait)\")\n",
    "        spinner = spinning_cursor()\n",
    "        while  runningCall :\n",
    "            r = ws_get_csv_job_status(config)\n",
    "            if (r.json()['data'] == 'complete') :\n",
    "                runningCall = 0\n",
    "            if (r.json()['data'] == 'error') :\n",
    "                runningCall = 0\n",
    "                print(\"Error while loading CSV -- column number mismatch\")\n",
    "                return 0\n",
    "            sys.stdout.write(next(spinner))\n",
    "            sys.stdout.flush()\n",
    "            sys.stdout.write('\\b')\n",
    "            time.sleep(SPINNING_RATE)\n",
    "\n",
    "    r = ws_create_log(config)\n",
    "    if (r.status_code != 200): return r\n",
    "    else :\n",
    "        # wait until async call is completed\n",
    "        config['job_key']= r.json()['data']\n",
    "        runningCall = 1\n",
    "        print(\"Process Mining: refreshing model (please wait)\")\n",
    "        spinner = spinning_cursor()\n",
    "        while  runningCall :\n",
    "            r = ws_get_csv_job_status(config)\n",
    "            if (r.json()['data'] == 'complete') :\n",
    "                runningCall = 0\n",
    "            if (r.json()['data'] == 'error') :\n",
    "                runningCall = 0\n",
    "                print(\"Error while creating the log\")\n",
    "                return 0\n",
    "            sys.stdout.write(next(spinner))\n",
    "            sys.stdout.flush()\n",
    "            sys.stdout.write('\\b')\n",
    "            time.sleep(SPINNING_RATE)\n",
    "    return r\n",
    "\n",
    "# List all the dashboards of a given process mining project\n",
    "def ws_get_dashboards(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/analytics/integration/dashboard/%s/list\" % (config['url'], config['project_key'])\n",
    "    r = requests.get(url, verify=False, params=params, headers=headers)\n",
    "    if (r.status_code == 200):\n",
    "        values = r.json()\n",
    "        return values['data']['dashboards']\n",
    "    else:\n",
    "        return r\n",
    "\n",
    "# Returns only the table widgets in the dashboard_name\n",
    "def ws_get_widgets(config):\n",
    "    dashboards = ws_get_dashboards(config)\n",
    "    dashboard = 0\n",
    "    for aDashboard in dashboards:\n",
    "        if (aDashboard['name'] == config['dashboard_name']):\n",
    "            dashboard = aDashboard\n",
    "\n",
    "    if dashboard == 0 :\n",
    "        print(\"ERROR: dashboard does not exist\")\n",
    "        return 0\n",
    "    dashboard_id = dashboard['id']\n",
    "\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/analytics/integration/dashboard/%s/%s/list\" % (config['url'], config['project_key'], dashboard_id)\n",
    "\n",
    "    r = requests.get(url, verify=False, params=params, headers=headers )\n",
    "    widgets = r.json()\n",
    "    return widgets['data']['widgets']\n",
    "\n",
    "def ws_get_widget_values(config):\n",
    "    dashboard_id = config.get('dashboard_id')\n",
    "    if dashboard_id is None:\n",
    "        # add the dashboard_id to the widget data\n",
    "        print(\"....searching dashboard_id\")\n",
    "        dashboards = ws_get_dashboards(config)\n",
    "        dashboard = 0\n",
    "        for aDashboard in dashboards:\n",
    "            if (aDashboard['name'] == config['dashboard_name']):\n",
    "                dashboard = aDashboard\n",
    "        if (dashboard == 0) :\n",
    "            print(\"ERROR: dashboard does not exist\")\n",
    "            return 0\n",
    "        else:\n",
    "            # Store the dashboard id in the widget to avoid calling again the rest API to retrieve the dashboard by name\n",
    "            config['dashboard_id'] = dashboard['id']\n",
    "\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/analytics/integration/dashboard/%s/%s/%s/retrieve\" % (config['url'],\n",
    "                                                                        config['project_key'],\n",
    "                                                                        config['dashboard_id'],\n",
    "                                                                        config['widget_id'])\n",
    "\n",
    "    r = requests.get(url, verify=False, params=params, headers=headers )\n",
    "    values = r.json()\n",
    "    return values['data']\n",
    "\n",
    "def ws_create_update_variables(config, variablesArray):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/processes/%s/variables\" % (config['url'], config['project_key'])\n",
    "    r = requests.post(url, verify=False, data=json.dumps(variablesArray), params=params, headers=headers )\n",
    "    return r\n",
    "\n",
    "def ws_get_variable(config, variablename):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "    \n",
    "    url = \"%s/integration/processes/%s/variables/%s\" % (config['url'], config['project_key'], variablename)\n",
    "    r = requests.get(url, verify=False, params=params, headers=headers )\n",
    "    if (r.status_code == 200):\n",
    "        values = r.json()\n",
    "        return values['data']\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def ws_get_variables(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/processes/%s/variables\" % (config['url'], config['project_key'])\n",
    "    r = requests.get(url, verify=False, params=params, headers=headers )\n",
    "    if (r.status_code == 200):\n",
    "        values = r.json()\n",
    "        return values['data']\n",
    "    else:\n",
    "        print(\"Error: ws_get_variables %s\" % r.json()['data'])\n",
    "        return []\n",
    "\n",
    "def ws_delete_variable(config, variablename):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/processes/%s/variables/%s\" % (config['url'], config['project_key'], variablename)\n",
    "    r = requests.delete(url, verify=False, params=params, headers=headers )\n",
    "    return r.status_code\n",
    "\n",
    "def ws_delete_variables(config):\n",
    "    variables = ws_get_variables(config)\n",
    "    for variable in variables:\n",
    "        ws_delete_variable(config, variable['name'])\n",
    "\n",
    "def ws_get_processes(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/processes\" % (config['url'])\n",
    "    r = requests.get(url, verify=False, params=params, headers=headers )\n",
    "    if (r.status_code == 200):\n",
    "        values = r.json()\n",
    "        return values['data']\n",
    "    else:\n",
    "        return r\n",
    "\n",
    "def ws_get_project_info(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/processes/%s\" % (config['url'], config['project_key'])\n",
    "\n",
    "    return(requests.get(url, verify=False, params=params, headers=headers ))\n",
    "\n",
    "\n",
    "def ws_get_project_meta_info(config):\n",
    "    headers={}\n",
    "    params={}\n",
    "    init_params_headers(config, headers, params)\n",
    "\n",
    "    url = \"%s/integration/csv/%s/meta\" % (config['url'], config['project_key'])\n",
    "\n",
    "    r = requests.get(url, verify=False, params=params, headers=headers )\n",
    "    if (r.status_code == 200):\n",
    "        values = r.json()\n",
    "        return values['data']\n",
    "    else:\n",
    "        return r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....searching dashboard_id\n",
      "3 current alerts for widget alerts-widget-1 in dashboard Alerts Dashboard\n",
      "Loading history file: alert_history_Alerts Dashboard_alerts-widget-1.csv\n",
      "Loading summary file: alert_summary_Alerts Dashboard_alerts-widget-1.csv\n",
      "3 alerts already CLOSED\n",
      "3 alerts still PENDING\n",
      "Progression rate: 0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime as dt, timedelta\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "def load_history_df(history_file):\n",
    "    try: \n",
    "        histo_df = pd.read_csv(history_file, dtype=str)\n",
    "        print('Loading history file: %s' % history_file)\n",
    "        return histo_df\n",
    "    except:\n",
    "        # First call: Create the historical CSV from scratch at the end\n",
    "        print('First time execution, no history df')\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def manageAlertsFromWidget(myconfig):\n",
    "\n",
    "    # Example of a configuration object that could be stored as a config file passed as\n",
    "    # a parameter of the program\n",
    "    # custom_data are added to the alert fetched from the widget. They can be used to manage\n",
    "    # some business logic with the alerts received\n",
    "    #if (len(argv)==2):\n",
    "    #    try:\n",
    "    #        with open(argv[1], 'r') as file:\n",
    "    #            myconfig = json.load(file)\n",
    "    #            print('Loading config file')\n",
    "    #           print(myconfig)\n",
    "    #    except:\n",
    "    #        print('Error config file not found')\n",
    "    #else:\n",
    "    #    myconfig = {\n",
    "    #        \"url\":\"https://ProcessMining.com\",\n",
    "    #        \"user_id\": \"john.smith\",\n",
    "    #        \"api_key\":\"8a5kga87eqvd1180\",\n",
    "    #        \"project_key\": \"procure-to-pay\",\n",
    "    #        \"org_key\": \"\",\n",
    "    #        \"dashboard_name\": \"Alerts Dashboard\",\n",
    "    #        \"widget_id\": \"alerts-widget-1\",\n",
    "    #        \"custom_data\": {\"Status\":\"False\", \"Custom2\":\"default\"}\n",
    "    #    }\n",
    "\n",
    "        \n",
    "    ws_post_sign(myconfig)\n",
    "\n",
    "    # Get the current values from the widget\n",
    "    widgetAlerts = ws_get_widget_values(myconfig)\n",
    "    print(\"%d current alerts for widget %s in dashboard %s\" % (len(widgetAlerts), myconfig['widget_id'], myconfig['dashboard_name']))     \n",
    "    widget_df = pd.DataFrame(widgetAlerts)\n",
    "\n",
    "    # if the alert_history file does not exist, create it\n",
    "    history_file = 'alert_history_' + myconfig['dashboard_name'] + '_' + myconfig['widget_id']+'.csv'\n",
    "    summary_file = 'alert_summary_' + myconfig['dashboard_name'] + '_' + myconfig['widget_id']+'.csv'\n",
    "\n",
    "    # Load alert history for this widget\n",
    "    histo_df = load_history_df(history_file)\n",
    "    if (len(histo_df) == 0):\n",
    "        # First time execution\n",
    "        if (len(widget_df) ==  0): # No alerts in the widget\n",
    "            print('Empty widget, nothing to do')\n",
    "            return\n",
    "        else: # create the first history file and summary file and quit\n",
    "            histo_df = pd.DataFrame(widgetAlerts)\n",
    "            histo_df['alert_status'] = 'NEW'\n",
    "            histo_df['alert_creation_date'] = dt.now().isoformat()\n",
    "            histo_df['alert_closed_date'] = ''\n",
    "            histo_df.to_csv(history_file, index=None)\n",
    "            summary = {\n",
    "                'update_date': dt.now().isoformat(),\n",
    "                #'last_update_date': 0,\n",
    "                #'last_new': 0,\n",
    "                #'last_pending': 0,\n",
    "                #'last_closed': 0,\n",
    "                'new': len(histo_df),\n",
    "                'pending': 0,\n",
    "                'closed': 0,\n",
    "                'new_to_pending': 0,\n",
    "                'new_to_closed': 0,\n",
    "                'pending_to_closed': 0,\n",
    "                'pending_to_pending': 0,\n",
    "                'any_to_closed': 0,\n",
    "                'progression_rate': 0\n",
    "            }\n",
    "            summary_df = pd.DataFrame([summary])\n",
    "            summary_df.to_csv(summary_file, index=None)\n",
    "            return\n",
    "\n",
    "    # Load summary history for this widget\n",
    "    try: \n",
    "        summary_df = pd.read_csv(summary_file)\n",
    "        print('Loading summary file: %s' % summary_file)\n",
    "        last_summary = summary_df.loc[len(summary_df) - 1]\n",
    "    except:\n",
    "        # First call: Create the summary df \n",
    "        summary_df = pd.DataFrame()\n",
    "        last_summary = {\n",
    "            'update_date': 0,\n",
    "            'new': 0,\n",
    "            'pending': 0,\n",
    "            'closed': 0,\n",
    "            'new_to_pending': 0,\n",
    "            'new_to_closed': 0,\n",
    "            'pending_to_closed': 0,\n",
    "            'pending_to_pending': 0,\n",
    "            'any_to_closed': 0,\n",
    "            'progression_rate': 0\n",
    "        }\n",
    "    summary = {\n",
    "        'update_date': dt.now().isoformat(),\n",
    "        'new': 0,\n",
    "        'pending': 0,\n",
    "        'closed': last_summary['closed'],\n",
    "        'new_to_pending': 0,\n",
    "        'new_to_closed': 0,\n",
    "        'pending_to_closed': 0,\n",
    "        'pending_to_pending': 0,\n",
    "        'any_to_closed': 0,\n",
    "        'progression_rate': 0\n",
    "    }\n",
    "\n",
    "    # Collect the number of alerts in history for each status\n",
    "    histo_closed_df = histo_df[histo_df['alert_status'] ==  'CLOSED']\n",
    "    final_df = histo_closed_df # we keep the closed alerts\n",
    "\n",
    "    # History: CLOSED alerts in histo are still closed\n",
    "    if (last_summary['closed']):\n",
    "        print('%d alerts already CLOSED' % last_summary['closed'])\n",
    "\n",
    "    # If widget is empty, all the alerts are CLOSED\n",
    "    if (len(widget_df) == 0):\n",
    "        print(\"Empty widget, all alerts are closed\")\n",
    "        summary['new'] = 0\n",
    "        summary['pending'] = 0\n",
    "        summary['closed'] = len(histo_df)\n",
    "        summary['new_to_closed'] = last_summary['new']\n",
    "        summary['pending_to_closed'] = last_summary['pending']\n",
    "        summary['any_to_closed'] = summary['new_to_closed'] + summary['pending_to_closed']\n",
    "        summary['pending_to_pending'] = 0\n",
    "\n",
    "        histo_any_to_close_df = histo_df[histo_df['alert_status'] != 'CLOSED']\n",
    "        histo_any_to_close_df['alert_closed_date'] = dt.now().isoformat()\n",
    "        histo_any_to_close_df['alert_status'] =  'CLOSED'\n",
    "        final_df = pd.concat([final_df, histo_any_to_close_df])\n",
    "        final_df.to_csv(history_file, index=None)\n",
    "\n",
    "\n",
    "    else: # Widget is not empty\n",
    "        \n",
    "        # We remove CLOSED alerts from histo_df, such that we can add same alerts again if they appear in the widget\n",
    "        histo_not_closed_df = histo_df[histo_df['alert_status'] != 'CLOSED']\n",
    "\n",
    "        # NEW alerts in the widget\n",
    "        widget_new_df = widget_df.merge(histo_not_closed_df, on=widget_df.columns.tolist(), how='left', indicator='alert_is_present_on')\n",
    "        # Alerts in the widget with 'exist'==left_only are NEW\n",
    "        widget_new_df = widget_new_df[widget_new_df['alert_is_present_on'] == 'left_only']\n",
    "        summary['new'] = len(widget_new_df)\n",
    "        if (summary['new']):\n",
    "            widget_new_df['alert_status'] = 'NEW'\n",
    "            widget_new_df['alert_creation_date'] = dt.now().isoformat()\n",
    "            widget_new_df['alert_closed_date'] = ''\n",
    "            print('%d new alerts' % summary['new'])\n",
    "            final_df = pd.concat([final_df, widget_new_df.drop(columns=['alert_is_present_on'])])\n",
    "\n",
    "\n",
    "        # Process alerts that are in the HISTORY\n",
    "        histo_not_closed_df = histo_not_closed_df.merge(widget_df, on=widget_df.columns.tolist(), how='left', indicator='alert_is_present_on')\n",
    "        # NEW to PENDING  \n",
    "        histo_new_to_pending_df = histo_not_closed_df.query('alert_status==\"NEW\" & alert_is_present_on==\"both\"')\n",
    "        summary['new_to_pending'] = len(histo_new_to_pending_df)\n",
    "        summary['pending'] =  summary['new_to_pending']                                \n",
    "        if (summary['new_to_pending']):\n",
    "            print('%d alerts moved from NEW to PENDING' % summary['new_to_pending'])\n",
    "            histo_new_to_pending_df['alert_status'] = 'PENDING'\n",
    "            final_df = pd.concat([final_df, histo_new_to_pending_df.drop(columns=['alert_is_present_on'])])\n",
    "\n",
    "        # NEW to CLOSE\n",
    "        histo_new_to_close_df = histo_not_closed_df.query('alert_status==\"NEW\" & alert_is_present_on==\"left_only\"')\n",
    "        summary['new_to_closed'] = len(histo_new_to_close_df)\n",
    "        summary['closed'] += summary['new_to_closed']                              \n",
    "        if (summary['new_to_closed']):\n",
    "            print('%d alerts moved from NEW to CLOSED' % summary['new_to_closed'])\n",
    "            histo_new_to_close_df['alert_status'] = 'CLOSED'\n",
    "            histo_new_to_close_df['alert_closed_date'] = dt.now().isoformat()\n",
    "            final_df = pd.concat([final_df, histo_new_to_close_df.drop(columns=['alert_is_present_on'])]) \n",
    "\n",
    "        # PENDING to PENDING\n",
    "        histo_pending_to_pending_df = histo_not_closed_df.query('alert_status==\"PENDING\" & alert_is_present_on==\"both\"')\n",
    "        summary['pending_to_pending'] = len(histo_pending_to_pending_df)\n",
    "        summary['pending'] += summary['pending_to_pending']\n",
    "        if (summary['pending_to_pending']):\n",
    "            print('%d alerts still PENDING' % summary['pending_to_pending'])\n",
    "            final_df = pd.concat([final_df, histo_pending_to_pending_df.drop(columns=['alert_is_present_on'])])\n",
    "\n",
    "        # PENDING to CLOSE\n",
    "        histo_pending_to_close_df = histo_not_closed_df.query('alert_status==\"PENDING\" & alert_is_present_on==\"left_only\"')\n",
    "        summary['pending_to_closed'] = len(histo_pending_to_close_df)\n",
    "        summary['closed'] += summary['pending_to_closed']                              \n",
    "\n",
    "        if (summary['pending_to_closed']):\n",
    "            print('%d alerts moved from PENDING to CLOSE' % summary['pending_to_closed'])\n",
    "            final_df = pd.concat([final_df, histo_pending_to_close_df.drop(columns=['alert_is_present_on'])]) \n",
    "\n",
    "    # Save final_df in the history file\n",
    "    final_df.to_csv(history_file, index=None)\n",
    "\n",
    "    # Add the summary object to the summary dataframe\n",
    "    if (last_summary['pending'] + last_summary['new']):\n",
    "        summary['progression_rate'] = summary['any_to_closed'] / (last_summary['pending'] + last_summary['new'])\n",
    "    else:\n",
    "        summary['progression_rate'] = 0.0\n",
    "    print('Progression rate: %d' % summary['progression_rate'])\n",
    "    summary_df.loc[len(summary_df)] = summary\n",
    "    summary_df.to_csv(summary_file, index=None)\n",
    "\n",
    "config = {\n",
    "    \"url\":\"https://pharoses1.fyre.ibm.com\",\n",
    "    \"user_id\": \"task.miner\",\n",
    "    \"api_key\":\"8a5kga87eqvd1180\",\n",
    "    \"project_key\": \"procure-to-pay\",\n",
    "    \"org_key\": \"\",\n",
    "    \"dashboard_name\": \"Alerts Dashboard\",\n",
    "    \"widget_id\": \"alerts-widget-1\",\n",
    "    \"custom_data\": {\"Status\":\"False\", \"Custom2\":\"default\"}\n",
    "}\n",
    "\n",
    "\n",
    "manageAlertsFromWidget(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
